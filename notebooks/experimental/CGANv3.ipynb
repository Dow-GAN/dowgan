{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec40ebef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Reshape, Flatten, Concatenate, Conv1D, Conv1DTranspose, LeakyReLU, BatchNormalization, Embedding, Multiply\n",
    "from keras.layers import Dropout\n",
    "from keras.utils import to_categorical\n",
    "import keras\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9200da5",
   "metadata": {},
   "source": [
    "### Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c5f4d98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(522, 20)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CSV path\n",
    "folder_name = \"data\"\n",
    "file_name = \"hungary_chickenpox.csv\"\n",
    "path = os.path.join(folder_name, file_name)\n",
    "#Load CSV into Dataframe\n",
    "df = pd.read_csv(path,sep=',')\n",
    "df = df.drop(columns = ['Date'])\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b5b003d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 20)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "col_names = list(df.columns)\n",
    "# perform one-hot encoding using LabelBinarizer\n",
    "lb = LabelBinarizer()\n",
    "targets = lb.fit_transform(col_names)\n",
    "targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ccb92fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "# fit the scaler to data\n",
    "scaler.fit(df)\n",
    "\n",
    "# transform the data to MinMax scale\n",
    "df = scaler.transform(df)\n",
    "\n",
    "# create a new DataFrame with the scaled data\n",
    "df = pd.DataFrame(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f898ff9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>512</th>\n",
       "      <th>513</th>\n",
       "      <th>514</th>\n",
       "      <th>515</th>\n",
       "      <th>516</th>\n",
       "      <th>517</th>\n",
       "      <th>518</th>\n",
       "      <th>519</th>\n",
       "      <th>520</th>\n",
       "      <th>521</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.298539</td>\n",
       "      <td>-0.344468</td>\n",
       "      <td>-0.599165</td>\n",
       "      <td>-0.319415</td>\n",
       "      <td>-0.490605</td>\n",
       "      <td>-0.273486</td>\n",
       "      <td>-0.361169</td>\n",
       "      <td>-0.519833</td>\n",
       "      <td>-0.503132</td>\n",
       "      <td>-0.524008</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.883090</td>\n",
       "      <td>-0.816284</td>\n",
       "      <td>-0.862213</td>\n",
       "      <td>-0.645094</td>\n",
       "      <td>-0.933194</td>\n",
       "      <td>-0.603340</td>\n",
       "      <td>-0.820459</td>\n",
       "      <td>-0.853862</td>\n",
       "      <td>-0.874739</td>\n",
       "      <td>0.081420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.185567</td>\n",
       "      <td>-0.381443</td>\n",
       "      <td>-0.546392</td>\n",
       "      <td>-0.494845</td>\n",
       "      <td>-0.195876</td>\n",
       "      <td>-0.216495</td>\n",
       "      <td>0.061856</td>\n",
       "      <td>-0.237113</td>\n",
       "      <td>-0.113402</td>\n",
       "      <td>-0.164948</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.927835</td>\n",
       "      <td>-0.958763</td>\n",
       "      <td>-0.948454</td>\n",
       "      <td>-0.845361</td>\n",
       "      <td>-0.845361</td>\n",
       "      <td>-0.876289</td>\n",
       "      <td>-0.597938</td>\n",
       "      <td>-0.927835</td>\n",
       "      <td>-0.762887</td>\n",
       "      <td>-0.567010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.781022</td>\n",
       "      <td>-0.781022</td>\n",
       "      <td>-0.773723</td>\n",
       "      <td>-0.686131</td>\n",
       "      <td>-0.613139</td>\n",
       "      <td>-0.437956</td>\n",
       "      <td>-0.605839</td>\n",
       "      <td>-0.532847</td>\n",
       "      <td>-0.583942</td>\n",
       "      <td>-0.058394</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.992701</td>\n",
       "      <td>-0.948905</td>\n",
       "      <td>-0.927007</td>\n",
       "      <td>-0.810219</td>\n",
       "      <td>-0.897810</td>\n",
       "      <td>-0.700730</td>\n",
       "      <td>-0.773723</td>\n",
       "      <td>-0.890511</td>\n",
       "      <td>-0.941606</td>\n",
       "      <td>-0.642336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.276753</td>\n",
       "      <td>-0.321033</td>\n",
       "      <td>-0.365314</td>\n",
       "      <td>-0.070111</td>\n",
       "      <td>-0.357934</td>\n",
       "      <td>0.121771</td>\n",
       "      <td>0.416974</td>\n",
       "      <td>0.284133</td>\n",
       "      <td>0.261993</td>\n",
       "      <td>0.601476</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.985240</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.948339</td>\n",
       "      <td>-0.985240</td>\n",
       "      <td>-0.955720</td>\n",
       "      <td>-0.926199</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.763838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.047887</td>\n",
       "      <td>0.126761</td>\n",
       "      <td>-0.476056</td>\n",
       "      <td>-0.740845</td>\n",
       "      <td>-0.419718</td>\n",
       "      <td>0.064789</td>\n",
       "      <td>-0.166197</td>\n",
       "      <td>-0.211268</td>\n",
       "      <td>-0.492958</td>\n",
       "      <td>-0.059155</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.926761</td>\n",
       "      <td>-0.909859</td>\n",
       "      <td>-0.836620</td>\n",
       "      <td>-0.921127</td>\n",
       "      <td>-0.983099</td>\n",
       "      <td>-0.780282</td>\n",
       "      <td>-0.808451</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.938028</td>\n",
       "      <td>-0.785915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.577889</td>\n",
       "      <td>-0.467337</td>\n",
       "      <td>-0.698492</td>\n",
       "      <td>-0.608040</td>\n",
       "      <td>-0.658291</td>\n",
       "      <td>-0.738693</td>\n",
       "      <td>-0.346734</td>\n",
       "      <td>-0.437186</td>\n",
       "      <td>-0.346734</td>\n",
       "      <td>-0.356784</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.979899</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.969849</td>\n",
       "      <td>-0.969849</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.969849</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.959799</td>\n",
       "      <td>-0.849246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.658537</td>\n",
       "      <td>-0.378049</td>\n",
       "      <td>0.134146</td>\n",
       "      <td>-0.365854</td>\n",
       "      <td>0.158537</td>\n",
       "      <td>-0.097561</td>\n",
       "      <td>0.219512</td>\n",
       "      <td>0.353659</td>\n",
       "      <td>0.439024</td>\n",
       "      <td>0.134146</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.841463</td>\n",
       "      <td>-0.780488</td>\n",
       "      <td>-0.756098</td>\n",
       "      <td>-0.658537</td>\n",
       "      <td>-0.890244</td>\n",
       "      <td>-0.804878</td>\n",
       "      <td>-0.975610</td>\n",
       "      <td>-0.914634</td>\n",
       "      <td>-0.987805</td>\n",
       "      <td>-0.865854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.325967</td>\n",
       "      <td>-0.226519</td>\n",
       "      <td>-0.071823</td>\n",
       "      <td>0.259669</td>\n",
       "      <td>0.447514</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.303867</td>\n",
       "      <td>0.933702</td>\n",
       "      <td>0.160221</td>\n",
       "      <td>0.701657</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.878453</td>\n",
       "      <td>-0.878453</td>\n",
       "      <td>-0.922652</td>\n",
       "      <td>-0.856354</td>\n",
       "      <td>-0.745856</td>\n",
       "      <td>-0.834254</td>\n",
       "      <td>-0.668508</td>\n",
       "      <td>-0.922652</td>\n",
       "      <td>-0.900552</td>\n",
       "      <td>0.082873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.236641</td>\n",
       "      <td>-0.358779</td>\n",
       "      <td>0.458015</td>\n",
       "      <td>-0.183206</td>\n",
       "      <td>0.312977</td>\n",
       "      <td>0.198473</td>\n",
       "      <td>-0.015267</td>\n",
       "      <td>0.053435</td>\n",
       "      <td>0.480916</td>\n",
       "      <td>-0.091603</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.977099</td>\n",
       "      <td>-0.916031</td>\n",
       "      <td>-0.977099</td>\n",
       "      <td>-0.931298</td>\n",
       "      <td>-0.954198</td>\n",
       "      <td>-0.893130</td>\n",
       "      <td>-0.809160</td>\n",
       "      <td>-0.969466</td>\n",
       "      <td>-0.923664</td>\n",
       "      <td>-0.534351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.657143</td>\n",
       "      <td>-0.733333</td>\n",
       "      <td>-0.514286</td>\n",
       "      <td>-0.600000</td>\n",
       "      <td>-0.619048</td>\n",
       "      <td>-0.580952</td>\n",
       "      <td>-0.619048</td>\n",
       "      <td>-0.428571</td>\n",
       "      <td>-0.428571</td>\n",
       "      <td>-0.676190</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.990476</td>\n",
       "      <td>-0.990476</td>\n",
       "      <td>-0.990476</td>\n",
       "      <td>-0.971429</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.904762</td>\n",
       "      <td>-0.819048</td>\n",
       "      <td>-0.980952</td>\n",
       "      <td>-0.838095</td>\n",
       "      <td>-0.638095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.160714</td>\n",
       "      <td>-0.285714</td>\n",
       "      <td>-0.428571</td>\n",
       "      <td>-0.437500</td>\n",
       "      <td>-0.455357</td>\n",
       "      <td>-0.151786</td>\n",
       "      <td>-0.214286</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.401786</td>\n",
       "      <td>0.053571</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.901786</td>\n",
       "      <td>-0.901786</td>\n",
       "      <td>-0.875000</td>\n",
       "      <td>-0.875000</td>\n",
       "      <td>-0.785714</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>-0.696429</td>\n",
       "      <td>-0.732143</td>\n",
       "      <td>-0.758929</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-0.287500</td>\n",
       "      <td>-0.375000</td>\n",
       "      <td>-0.425000</td>\n",
       "      <td>-0.325000</td>\n",
       "      <td>-0.387500</td>\n",
       "      <td>0.212500</td>\n",
       "      <td>-0.300000</td>\n",
       "      <td>-0.125000</td>\n",
       "      <td>-0.425000</td>\n",
       "      <td>-0.087500</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.962500</td>\n",
       "      <td>-0.987500</td>\n",
       "      <td>-0.875000</td>\n",
       "      <td>-0.900000</td>\n",
       "      <td>-0.975000</td>\n",
       "      <td>-0.912500</td>\n",
       "      <td>-0.750000</td>\n",
       "      <td>-0.550000</td>\n",
       "      <td>-0.787500</td>\n",
       "      <td>-0.237500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-0.964286</td>\n",
       "      <td>-0.482143</td>\n",
       "      <td>-0.928571</td>\n",
       "      <td>-0.750000</td>\n",
       "      <td>-0.803571</td>\n",
       "      <td>-0.535714</td>\n",
       "      <td>-0.821429</td>\n",
       "      <td>-0.625000</td>\n",
       "      <td>-0.785714</td>\n",
       "      <td>-0.892857</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.892857</td>\n",
       "      <td>-0.910714</td>\n",
       "      <td>-0.892857</td>\n",
       "      <td>-0.553571</td>\n",
       "      <td>-0.839286</td>\n",
       "      <td>-0.767857</td>\n",
       "      <td>-0.678571</td>\n",
       "      <td>-0.928571</td>\n",
       "      <td>-0.625000</td>\n",
       "      <td>-0.053571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-0.174014</td>\n",
       "      <td>-0.345708</td>\n",
       "      <td>-0.271462</td>\n",
       "      <td>-0.503480</td>\n",
       "      <td>-0.424594</td>\n",
       "      <td>-0.322506</td>\n",
       "      <td>-0.447796</td>\n",
       "      <td>-0.174014</td>\n",
       "      <td>-0.480278</td>\n",
       "      <td>-0.396752</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.851508</td>\n",
       "      <td>-0.786543</td>\n",
       "      <td>-0.596288</td>\n",
       "      <td>-0.846868</td>\n",
       "      <td>-0.883991</td>\n",
       "      <td>-0.433875</td>\n",
       "      <td>-0.675174</td>\n",
       "      <td>-0.665893</td>\n",
       "      <td>-0.944316</td>\n",
       "      <td>0.187935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.148387</td>\n",
       "      <td>-0.380645</td>\n",
       "      <td>-0.574194</td>\n",
       "      <td>-0.148387</td>\n",
       "      <td>-0.187097</td>\n",
       "      <td>-0.238710</td>\n",
       "      <td>0.341935</td>\n",
       "      <td>-0.096774</td>\n",
       "      <td>0.496774</td>\n",
       "      <td>-0.122581</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.793548</td>\n",
       "      <td>-0.638710</td>\n",
       "      <td>-0.935484</td>\n",
       "      <td>-0.638710</td>\n",
       "      <td>-0.793548</td>\n",
       "      <td>-0.948387</td>\n",
       "      <td>-0.535484</td>\n",
       "      <td>-0.935484</td>\n",
       "      <td>-0.935484</td>\n",
       "      <td>-0.419355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-0.369458</td>\n",
       "      <td>-0.714286</td>\n",
       "      <td>-0.674877</td>\n",
       "      <td>-0.507389</td>\n",
       "      <td>-0.448276</td>\n",
       "      <td>-0.467980</td>\n",
       "      <td>-0.162562</td>\n",
       "      <td>-0.261084</td>\n",
       "      <td>-0.251232</td>\n",
       "      <td>-0.418719</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.931034</td>\n",
       "      <td>-0.724138</td>\n",
       "      <td>-0.852217</td>\n",
       "      <td>-0.931034</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.773399</td>\n",
       "      <td>-0.950739</td>\n",
       "      <td>-0.793103</td>\n",
       "      <td>-0.832512</td>\n",
       "      <td>-0.615764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-0.832061</td>\n",
       "      <td>-0.114504</td>\n",
       "      <td>-0.633588</td>\n",
       "      <td>-0.618321</td>\n",
       "      <td>-0.893130</td>\n",
       "      <td>-0.587786</td>\n",
       "      <td>-0.694656</td>\n",
       "      <td>-0.923664</td>\n",
       "      <td>-0.664122</td>\n",
       "      <td>-0.526718</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.679389</td>\n",
       "      <td>-0.938931</td>\n",
       "      <td>-0.938931</td>\n",
       "      <td>-0.618321</td>\n",
       "      <td>-0.862595</td>\n",
       "      <td>-0.938931</td>\n",
       "      <td>-0.648855</td>\n",
       "      <td>-0.786260</td>\n",
       "      <td>-0.984733</td>\n",
       "      <td>-0.587786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-0.588652</td>\n",
       "      <td>-0.248227</td>\n",
       "      <td>-0.744681</td>\n",
       "      <td>-0.702128</td>\n",
       "      <td>-0.333333</td>\n",
       "      <td>-0.234043</td>\n",
       "      <td>-0.546099</td>\n",
       "      <td>-0.063830</td>\n",
       "      <td>-0.361702</td>\n",
       "      <td>0.205674</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.560284</td>\n",
       "      <td>-0.673759</td>\n",
       "      <td>-0.957447</td>\n",
       "      <td>-0.843972</td>\n",
       "      <td>-0.687943</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.985816</td>\n",
       "      <td>-0.843972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-0.243478</td>\n",
       "      <td>-0.408696</td>\n",
       "      <td>-0.460870</td>\n",
       "      <td>-0.626087</td>\n",
       "      <td>-0.260870</td>\n",
       "      <td>-0.582609</td>\n",
       "      <td>0.330435</td>\n",
       "      <td>0.295652</td>\n",
       "      <td>-0.113043</td>\n",
       "      <td>-0.165217</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.704348</td>\n",
       "      <td>-0.739130</td>\n",
       "      <td>-0.817391</td>\n",
       "      <td>-0.791304</td>\n",
       "      <td>-0.782609</td>\n",
       "      <td>-0.043478</td>\n",
       "      <td>-0.452174</td>\n",
       "      <td>-0.852174</td>\n",
       "      <td>-0.278261</td>\n",
       "      <td>-0.104348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-0.370370</td>\n",
       "      <td>-0.759259</td>\n",
       "      <td>-0.592593</td>\n",
       "      <td>-0.712963</td>\n",
       "      <td>-0.444444</td>\n",
       "      <td>-0.444444</td>\n",
       "      <td>-0.351852</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>-0.611111</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.907407</td>\n",
       "      <td>-0.935185</td>\n",
       "      <td>-0.990741</td>\n",
       "      <td>-0.962963</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.907407</td>\n",
       "      <td>-0.916667</td>\n",
       "      <td>-0.907407</td>\n",
       "      <td>-0.981481</td>\n",
       "      <td>-0.768519</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows Ã— 522 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6    \\\n",
       "0  -0.298539 -0.344468 -0.599165 -0.319415 -0.490605 -0.273486 -0.361169   \n",
       "1  -0.185567 -0.381443 -0.546392 -0.494845 -0.195876 -0.216495  0.061856   \n",
       "2  -0.781022 -0.781022 -0.773723 -0.686131 -0.613139 -0.437956 -0.605839   \n",
       "3   0.276753 -0.321033 -0.365314 -0.070111 -0.357934  0.121771  0.416974   \n",
       "4  -0.047887  0.126761 -0.476056 -0.740845 -0.419718  0.064789 -0.166197   \n",
       "5  -0.577889 -0.467337 -0.698492 -0.608040 -0.658291 -0.738693 -0.346734   \n",
       "6   0.658537 -0.378049  0.134146 -0.365854  0.158537 -0.097561  0.219512   \n",
       "7   0.325967 -0.226519 -0.071823  0.259669  0.447514  1.000000  0.303867   \n",
       "8   0.236641 -0.358779  0.458015 -0.183206  0.312977  0.198473 -0.015267   \n",
       "9  -0.657143 -0.733333 -0.514286 -0.600000 -0.619048 -0.580952 -0.619048   \n",
       "10  0.160714 -0.285714 -0.428571 -0.437500 -0.455357 -0.151786 -0.214286   \n",
       "11 -0.287500 -0.375000 -0.425000 -0.325000 -0.387500  0.212500 -0.300000   \n",
       "12 -0.964286 -0.482143 -0.928571 -0.750000 -0.803571 -0.535714 -0.821429   \n",
       "13 -0.174014 -0.345708 -0.271462 -0.503480 -0.424594 -0.322506 -0.447796   \n",
       "14 -0.148387 -0.380645 -0.574194 -0.148387 -0.187097 -0.238710  0.341935   \n",
       "15 -0.369458 -0.714286 -0.674877 -0.507389 -0.448276 -0.467980 -0.162562   \n",
       "16 -0.832061 -0.114504 -0.633588 -0.618321 -0.893130 -0.587786 -0.694656   \n",
       "17 -0.588652 -0.248227 -0.744681 -0.702128 -0.333333 -0.234043 -0.546099   \n",
       "18 -0.243478 -0.408696 -0.460870 -0.626087 -0.260870 -0.582609  0.330435   \n",
       "19 -0.370370 -0.759259 -0.592593 -0.712963 -0.444444 -0.444444 -0.351852   \n",
       "\n",
       "         7         8         9    ...       512       513       514       515  \\\n",
       "0  -0.519833 -0.503132 -0.524008  ... -0.883090 -0.816284 -0.862213 -0.645094   \n",
       "1  -0.237113 -0.113402 -0.164948  ... -0.927835 -0.958763 -0.948454 -0.845361   \n",
       "2  -0.532847 -0.583942 -0.058394  ... -0.992701 -0.948905 -0.927007 -0.810219   \n",
       "3   0.284133  0.261993  0.601476  ... -0.985240 -1.000000 -1.000000 -0.948339   \n",
       "4  -0.211268 -0.492958 -0.059155  ... -0.926761 -0.909859 -0.836620 -0.921127   \n",
       "5  -0.437186 -0.346734 -0.356784  ... -0.979899 -1.000000 -0.969849 -0.969849   \n",
       "6   0.353659  0.439024  0.134146  ... -0.841463 -0.780488 -0.756098 -0.658537   \n",
       "7   0.933702  0.160221  0.701657  ... -0.878453 -0.878453 -0.922652 -0.856354   \n",
       "8   0.053435  0.480916 -0.091603  ... -0.977099 -0.916031 -0.977099 -0.931298   \n",
       "9  -0.428571 -0.428571 -0.676190  ... -0.990476 -0.990476 -0.990476 -0.971429   \n",
       "10  0.000000 -0.401786  0.053571  ... -0.901786 -0.901786 -0.875000 -0.875000   \n",
       "11 -0.125000 -0.425000 -0.087500  ... -0.962500 -0.987500 -0.875000 -0.900000   \n",
       "12 -0.625000 -0.785714 -0.892857  ... -0.892857 -0.910714 -0.892857 -0.553571   \n",
       "13 -0.174014 -0.480278 -0.396752  ... -0.851508 -0.786543 -0.596288 -0.846868   \n",
       "14 -0.096774  0.496774 -0.122581  ... -0.793548 -0.638710 -0.935484 -0.638710   \n",
       "15 -0.261084 -0.251232 -0.418719  ... -0.931034 -0.724138 -0.852217 -0.931034   \n",
       "16 -0.923664 -0.664122 -0.526718  ... -0.679389 -0.938931 -0.938931 -0.618321   \n",
       "17 -0.063830 -0.361702  0.205674  ... -1.000000 -1.000000 -0.560284 -0.673759   \n",
       "18  0.295652 -0.113043 -0.165217  ... -0.704348 -0.739130 -0.817391 -0.791304   \n",
       "19 -0.500000 -0.611111 -0.500000  ... -0.907407 -0.935185 -0.990741 -0.962963   \n",
       "\n",
       "         516       517       518       519       520       521  \n",
       "0  -0.933194 -0.603340 -0.820459 -0.853862 -0.874739  0.081420  \n",
       "1  -0.845361 -0.876289 -0.597938 -0.927835 -0.762887 -0.567010  \n",
       "2  -0.897810 -0.700730 -0.773723 -0.890511 -0.941606 -0.642336  \n",
       "3  -0.985240 -0.955720 -0.926199 -1.000000 -1.000000 -0.763838  \n",
       "4  -0.983099 -0.780282 -0.808451 -1.000000 -0.938028 -0.785915  \n",
       "5  -1.000000 -1.000000 -0.969849 -1.000000 -0.959799 -0.849246  \n",
       "6  -0.890244 -0.804878 -0.975610 -0.914634 -0.987805 -0.865854  \n",
       "7  -0.745856 -0.834254 -0.668508 -0.922652 -0.900552  0.082873  \n",
       "8  -0.954198 -0.893130 -0.809160 -0.969466 -0.923664 -0.534351  \n",
       "9  -1.000000 -0.904762 -0.819048 -0.980952 -0.838095 -0.638095  \n",
       "10 -0.785714 -0.500000 -0.696429 -0.732143 -0.758929  0.000000  \n",
       "11 -0.975000 -0.912500 -0.750000 -0.550000 -0.787500 -0.237500  \n",
       "12 -0.839286 -0.767857 -0.678571 -0.928571 -0.625000 -0.053571  \n",
       "13 -0.883991 -0.433875 -0.675174 -0.665893 -0.944316  0.187935  \n",
       "14 -0.793548 -0.948387 -0.535484 -0.935484 -0.935484 -0.419355  \n",
       "15 -1.000000 -0.773399 -0.950739 -0.793103 -0.832512 -0.615764  \n",
       "16 -0.862595 -0.938931 -0.648855 -0.786260 -0.984733 -0.587786  \n",
       "17 -0.957447 -0.843972 -0.687943 -1.000000 -0.985816 -0.843972  \n",
       "18 -0.782609 -0.043478 -0.452174 -0.852174 -0.278261 -0.104348  \n",
       "19 -1.000000 -0.907407 -0.916667 -0.907407 -0.981481 -0.768519  \n",
       "\n",
       "[20 rows x 522 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transpose the data to get into proper shape\n",
    "df = df.T\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96be7ace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.29853862 -0.34446764 -0.59916493 ... -0.85386221 -0.87473904\n",
      "   0.08141962]\n",
      " [-0.18556701 -0.3814433  -0.54639175 ... -0.92783505 -0.7628866\n",
      "  -0.56701031]\n",
      " [-0.7810219  -0.7810219  -0.77372263 ... -0.89051095 -0.94160584\n",
      "  -0.64233577]\n",
      " ...\n",
      " [-0.58865248 -0.24822695 -0.74468085 ... -1.         -0.9858156\n",
      "  -0.84397163]\n",
      " [-0.24347826 -0.40869565 -0.46086957 ... -0.85217391 -0.27826087\n",
      "  -0.10434783]\n",
      " [-0.37037037 -0.75925926 -0.59259259 ... -0.90740741 -0.98148148\n",
      "  -0.76851852]]\n",
      "[[0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(522, (20, 20))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert the DataFrame to a NumPy array\n",
    "data = df.values\n",
    "\n",
    "# split the array into input features and targets\n",
    "x_train, y_train = data, targets\n",
    "\n",
    "# display the resulting input features and targets\n",
    "print(x_train)\n",
    "print(y_train)\n",
    "x_train.shape[1], y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70fa13f1",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "632b7a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# latent_dim is\n",
    "# num_classes is the number of features in the dataset\n",
    "# time_series_length is the total length of the training time series\n",
    "# epochs is the number of epoch iterations the training loop goes through\n",
    "# batch size is the batch of data that is fed into the training loop\n",
    "latent_dim = 100\n",
    "num_classes = len(y_train[0])\n",
    "time_series_length = len(x_train[1])\n",
    "\n",
    "epochs = 3000\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97dac448",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def build_generator(latent_dim, num_classes, time_series_length):\n",
    "    input_noise = Input(shape=(latent_dim,))\n",
    "    input_label = Input(shape=(num_classes,))\n",
    "    label_embedding = Dense(latent_dim)(input_label)\n",
    "    model_input = Multiply()([input_noise, label_embedding])\n",
    "    \n",
    "    x = Dense(128)(model_input)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    x = BatchNormalization(momentum=0.8)(x)\n",
    "    x = Dense(256)(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    x = BatchNormalization(momentum=0.8)(x)\n",
    "    x = Dense(time_series_length, activation=\"tanh\")(x)\n",
    "    model = Model([input_noise, input_label], x)\n",
    "    \n",
    "    return model\n",
    "\n",
    "generator = build_generator(latent_dim, num_classes, time_series_length)\n",
    "\n",
    "def build_discriminator(num_classes, time_series_length):\n",
    "    input_data = Input(shape=(time_series_length,))\n",
    "    input_label = Input(shape=(num_classes,))\n",
    "    label_embedding = Dense(time_series_length)(input_label)\n",
    "    label_embedding = Reshape((time_series_length, 1))(label_embedding)\n",
    "    model_input = Reshape((time_series_length, 1))(input_data)\n",
    "    model_input = Concatenate(axis=-1)([model_input, label_embedding])\n",
    "\n",
    "    x = Conv1D(64, kernel_size=3, strides=2, padding=\"same\")(model_input)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "\n",
    "    x = Conv1D(64, kernel_size=3, strides=2, padding=\"same\")(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "\n",
    "    x = Flatten()(x)\n",
    "    x = Dropout(0.4)(x)\n",
    "    x = Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "    model = Model([input_data, input_label], x, name=\"discriminator\")\n",
    "    return model\n",
    "\n",
    "discriminator = build_discriminator(num_classes, time_series_length)\n",
    "\n",
    "def build_cgan(generator, discriminator):\n",
    "    z = Input(shape=(latent_dim,))\n",
    "    label = Input(shape=(num_classes,))\n",
    "    time_series = generator([z, label])\n",
    "    validity = discriminator([time_series, label])\n",
    "    cgan = Model([z, label], validity)\n",
    "    \n",
    "    return cgan\n",
    "\n",
    "cgan = build_cgan(generator, discriminator)\n",
    "\n",
    "# Build the generator, discriminator, and CGAN models\n",
    "generator = build_generator(latent_dim, num_classes, time_series_length)\n",
    "discriminator = build_discriminator(num_classes, time_series_length)\n",
    "cgan = build_cgan(generator, discriminator)\n",
    "\n",
    "# Compile the discriminator and CGAN models\n",
    "discriminator.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5), metrics=['accuracy'])\n",
    "cgan.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7268c28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, D loss: 0.6829052567481995, G loss: 0.6869748830795288\n",
      "Epoch: 100, D loss: 0.28229880472645164, G loss: 0.9666293859481812\n",
      "Epoch: 200, D loss: 0.23564057052135468, G loss: 1.1572149991989136\n",
      "Epoch: 300, D loss: 0.3311660811305046, G loss: 1.1899998188018799\n",
      "Epoch: 400, D loss: 0.626584380865097, G loss: 0.8662759065628052\n",
      "Epoch: 500, D loss: 0.8553066253662109, G loss: 0.6243679523468018\n",
      "Epoch: 600, D loss: 0.8790485113859177, G loss: 0.5794399380683899\n",
      "Epoch: 700, D loss: 0.951704129576683, G loss: 0.5209857225418091\n",
      "Epoch: 800, D loss: 0.958622932434082, G loss: 0.5168349146842957\n",
      "Epoch: 900, D loss: 0.9701990634202957, G loss: 0.46041327714920044\n",
      "Epoch: 1000, D loss: 1.0112773030996323, G loss: 0.5709368586540222\n",
      "Epoch: 1100, D loss: 0.9958287328481674, G loss: 0.6502508521080017\n",
      "Epoch: 1200, D loss: 0.979327104985714, G loss: 0.4353025555610657\n",
      "Epoch: 1300, D loss: 0.8992889076471329, G loss: 0.6268472671508789\n",
      "Epoch: 1400, D loss: 0.9226656034588814, G loss: 0.6237623691558838\n",
      "Epoch: 1500, D loss: 0.7947418391704559, G loss: 0.7156371474266052\n",
      "Epoch: 1600, D loss: 0.8572488985955715, G loss: 0.8014370203018188\n",
      "Epoch: 1700, D loss: 0.8115599825978279, G loss: 0.6701523065567017\n",
      "Epoch: 1800, D loss: 0.7777628228068352, G loss: 0.7646030187606812\n",
      "Epoch: 1900, D loss: 0.659039281308651, G loss: 0.7101442813873291\n",
      "Epoch: 2000, D loss: 0.785822045058012, G loss: 0.6426851749420166\n",
      "Epoch: 2100, D loss: 0.6436423286795616, G loss: 0.7792335748672485\n",
      "Epoch: 2200, D loss: 0.6469058096408844, G loss: 1.0731463432312012\n",
      "Epoch: 2300, D loss: 0.6882512904703617, G loss: 0.8258888125419617\n",
      "Epoch: 2400, D loss: 0.6697523463517427, G loss: 1.036790370941162\n",
      "Epoch: 2500, D loss: 0.5815621465444565, G loss: 0.9738221168518066\n",
      "Epoch: 2600, D loss: 0.640520490705967, G loss: 1.2257429361343384\n",
      "Epoch: 2700, D loss: 0.7151762917637825, G loss: 0.9875649213790894\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    \n",
    "    idx = np.random.randint(0, x_train.shape[0], batch_size)\n",
    "\n",
    "    real_time_series = x_train[idx]\n",
    "\n",
    "    labels = y_train[idx]\n",
    "\n",
    "    noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
    "\n",
    "    gen_time_series = generator.predict([noise, labels],verbose=0)\n",
    "\n",
    "\n",
    "    # Train the discriminator\n",
    "    d_loss_real = discriminator.train_on_batch([real_time_series, labels], np.ones((batch_size, 1)))\n",
    "    d_loss_fake = discriminator.train_on_batch([gen_time_series, labels], np.zeros((batch_size, 1)))\n",
    "    d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "    #  Train Generator\n",
    "\n",
    "    # Generate a batch of noise and labels\n",
    "    noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
    "\n",
    "    sampled_labels = np.random.randint(0, num_classes, batch_size)\n",
    "    one_hot_labels = to_categorical(sampled_labels, num_classes=num_classes)\n",
    "    g_loss = cgan.train_on_batch([noise, one_hot_labels], np.ones((batch_size, 1)))\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch: {epoch}, D loss: {d_loss[0]}, G loss: {g_loss}\")\n",
    "        \n",
    "generator.save_weights(\"generator_weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13391e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_and_plot_data(num_instances, desired_class):\n",
    "    # create a figure and axis object\n",
    "    fig, ax = plt.subplots()\n",
    "    \n",
    "    # plot each instance on the same axis object\n",
    "    for i in range(num_instances):\n",
    "        latent_points = np.random.rand(1, latent_dim)\n",
    "        label = np.zeros((1, num_classes))\n",
    "        label[0, desired_class] = 1  # set the desired class to 1\n",
    "        generated_data = generator.predict([latent_points, label],verbose=0)\n",
    "        x = np.arange(0, generated_data.shape[1])\n",
    "        y = generated_data[0]\n",
    "        ax.plot(x, y, label=f\"Instance {i+1}\")\n",
    "    ax.set_xlabel('Time')\n",
    "    ax.set_ylabel('Scaled Cases')   \n",
    "    ax.set_title(f\"Generated Time Series Data for Class {desired_class}\")\n",
    "    ax.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ad6097",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_and_plot_data(5,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8761b41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_generator = build_generator(latent_dim, num_classes, time_series_length)\n",
    "new_generator.load_weights(\"generator_weights.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2965f03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b342f6e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38096711",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "    \n",
    "    # plot each instance on the same axis object\n",
    "latent_points = np.random.rand(1, latent_dim)\n",
    "label = np.zeros((1, num_classes))\n",
    "label[0, 5] = 1  # set the desired class to 1\n",
    "generated_data = new_generator.predict([latent_points, label],verbose=0)\n",
    "#         generated_data = scaler.inverse_transform(generated_data)\n",
    "x = df.T.index\n",
    "y = df.T[0]\n",
    "ax.plot(x, y, label='Original Data')\n",
    "x = np.arange(0, generated_data.shape[1])\n",
    "y = generated_data[0]\n",
    "ax.plot(x, y, label=f\"Generated Data\")\n",
    "ax.set_xlabel('Time')\n",
    "ax.set_ylabel('Scaled Cases')\n",
    "ax.set_title(f\"Generated Time Series Data for Class \")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4c85f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ab11ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
