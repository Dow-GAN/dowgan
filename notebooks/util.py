import plotly.graph_objects as go
import plotly.subplots as sp
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
import matplotlib.pyplot as plt
import seaborn as sns
import torch
import numpy as np
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

plt.rcParams['figure.max_open_warning'] = 50

def df_plot(df):
    ''' 
    Creates interactive plots for each column in the input DataFrame for data exploration. 

    This function iterates over each column in the DataFrame and generates an interactive plot 
    using Plotly. The function assumes that the DataFrame's index represents the time component
    for the x-axis, while each column's values are plotted on the y-axis.
    
    Parameters:
    df (pd.DataFrame): The input pandas DataFrame.
    '''
    
    for column in df:
        fig = go.Figure(data=go.Scatter(x=df.index, y=df[column], mode='markers'))
        fig.update_layout(
            title=column,
            xaxis_title="Time",
            yaxis_title="Value",
            autosize=False,
            width=1000,
            height=500,
            margin=dict(
                l=50,
                r=50,
                b=100,
                t=100,
                pad=4
            ),
            paper_bgcolor="LightSteelBlue",
        )
        fig.show()


def plot_losses(losses, title=None):
    ''' 
    Plot the loss curve for a list of losses over epochs. 

    Args:
        losses (list): A list of losses, one for each epoch.
        title (str, optional): The title of the plot. If provided, the title will be 
            "Training Loss". If not provided, the title will be "Training Loss".
    '''
    plt.figure(figsize=(10,5))
    plt.plot(losses)
    if title:
        plt.title(title)
    else:
        plt.title("Training Loss")
    plt.xlabel("Epoch")
    plt.ylabel("Loss")
    plt.show()



def plot_multiple_losses(losses_list, labels):
    '''
    Plot multiple loss curves
    
    Parameters:
        losses_list: list of lists containing loss values
        labels: list of strings with labels for each loss curve
    '''
    assert len(losses_list) == len(labels), "Number of losses must match number of labels"
    
    plt.figure(figsize=(10,5))
    
    # Iterate over each list of losses and plot
    for i in range(len(losses_list)):
        losses = [loss.detach().numpy() for loss in losses_list[i]]
        plt.plot(losses, label=labels[i])
    
    plt.title("Training Loss")
    plt.xlabel("Epoch")
    plt.ylabel("Loss")
    plt.legend()
    plt.show()

def generate_sequences(generator, recovery_network, num_samples, seq_length, condition):
    ''' 
    Generate a set of sequences using a generator model and a recovery network.

    This function takes a generator and a recovery network, as well as parameters 
    specifying the number of sequences to generate, their length, and an initial 
    condition. The function generates sequences by first creating random noise, 
    which is passed through the generator to create sequences. These sequences are 
    then passed through the recovery network to create the final output sequences.

    Parameters:
    generator (torch.nn.Module): The generator model used to create sequences.
    recovery_network (torch.nn.Module): The recovery network used to refine the generated sequences.
    num_samples (int): The number of sequences to generate.
    seq_length (int): The length of each sequence to generate.
    condition (torch.Tensor): A tensor providing the initial condition for the generator.

    Returns:
    recovered_sequences (numpy.ndarray): The sequences generated by the function. 
    '''
    
    # Set the generator to evaluation mode
    generator.eval()
    
    # adjust this according to your model's noise dimension
    noise_dim = condition.shape[1]
    
    # Generate a batch of noise
    noise = torch.randn(num_samples, seq_length, noise_dim)
    
    # Generate the data
    with torch.no_grad():  
        generated_sequences = generator(noise, condition)
        
    # Recovering the data
    recovered_sequences = recovery_network(generated_sequences)
    recovered_sequences = recovered_sequences[0].detach().numpy()

    return recovered_sequences
    
def generate_timeseries_sequences(generator, recovery_network, embedding_network, test_data, num_samples, seq_length):
    '''
    Generate a set of time-series sequences using a generator model, recovery network, and an embedding network.

    This function uses a trained generator model to generate time-series data. The function begins with 
    a segment of test data, generates a sequence of specified length based on that segment, then uses 
    the last data point of that sequence as the seed for the next sequence, and repeats this process 
    for a specified number of segments. 

    Parameters:
    generator (torch.nn.Module): The generator model used to create sequences.
    recovery_network (torch.nn.Module): The recovery network used to refine the generated sequences.
    embedding_network (torch.nn.Module): The embedding network used to transform a data point into an initial condition.
    test_data (numpy.ndarray or pandas.DataFrame): The test data used to generate the first sequence.
    num_samples (int): The number of sequences to generate.
    seq_length (int): The length of each sequence to generate.
    num_segments (int): The number of sequences or segments to generate.

    Returns:
    generated_data (numpy.ndarray): The complete time-series data generated by the function, consisting of num_segments sequences.
    '''
    
    # Empty list to hold the generated sequences
    generated_data = []

    # Process the initial test data tensor
    test_data_tensor = torch.from_numpy(test_data.values).unsqueeze(0).float()  
    condition = embedding_network(test_data_tensor)[:,0,:]

    # Loop over the number of segments
    for i in range(num_samples):
        # Generate a segment
        num_samples = 1
        segment = generate_sequences(generator, recovery_network, num_samples, seq_length, condition)
        generated_data.append(segment)
        last_sample = segment[-1]
        last_sample_tensor = torch.from_numpy(last_sample).unsqueeze(0).float()
        encode = embedding_network(last_sample_tensor).unsqueeze(0).float()
        condition = encode[:,0,:]
    # Concatenate all the segments into one sequence
    generated_data = np.concatenate(generated_data)
    
    return generated_data


def augment_timeseries_sequences(generator, recovery_network, embedding_network, test_data, num_samples, seq_length):
    '''
    Augment a set of time-series sequences using a generator model, recovery network, and an embedding network.

    This function uses a trained generator model to generate time-series data. It divides the input test data
    into multiple segments, and uses each segment to generate new sequences. The sequences are generated based on
    the first data point of each segment.

    Parameters:
    generator (torch.nn.Module): The generator model used to create sequences.
    recovery_network (torch.nn.Module): The recovery network used to refine the generated sequences.
    embedding_network (torch.nn.Module): The embedding network used to transform a data point into an initial condition.
    test_data (numpy.ndarray or pandas.DataFrame): The test data used to generate sequences.
    num_samples (int): The number of sequences to generate.
    seq_length (int): The length of each sequence to generate.

    Returns:
    generated_data (numpy.ndarray): The complete time-series data generated by the function.

    Note:
    The function requires that the generator, recovery network, and embedding network models are already trained.
    The function also assumes that the length of the test data is a multiple of the sequence length.
    '''

    # Empty list to hold the generated sequences
    generated_data = []

    # Number of segments based on the length of the test data
    num_segments = len(test_data) // seq_length

    # Loop over the number of segments
    for i in range(num_segments):
        # Extract the condition from the test data at intervals of seq_length
        test_data_segment = test_data[i*seq_length:(i+1)*seq_length]
        test_data_tensor = torch.from_numpy(test_data_segment.values).unsqueeze(0).float()  
        condition = embedding_network(test_data_tensor)[:,0,:]

        # Generate a segment
        segment = generate_sequences(generator, recovery_network, num_samples, seq_length, condition)
        generated_data.append(segment)

    # Concatenate all the segments into one sequence
    generated_data = np.concatenate(generated_data)

    return generated_data


def plot_sequences(df, embedding_network, recovery_network):
    """
    Function to plot and compare the original and decoded sequences.

    This function converts a DataFrame to a PyTorch tensor, passes it through the embedding
    and recovery networks, and then plots the first sequence of the original and recovered data for comparison.

    Parameters:
    df (pandas.DataFrame): The DataFrame to fetch the data from.
    embedding_network (torch.nn.Module): The embedding network used to transform the original sequence.
    recovery_network (torch.nn.Module): The recovery network used to recover the sequence from the embeddings.

    Returns:
    None. The function directly plots the original and decoded sequence using matplotlib.
    """

    # Convert DataFrame to tensor
    test_data_in = torch.tensor(df.values).float()

    # Add an extra dimension for batch size
    test_data_in = test_data_in.unsqueeze(0)

    # Pass the test data through the embedding and recovery networks
    encoded = embedding_network(test_data_in)
    decoded = recovery_network(encoded)
    test_data_in = test_data_in.detach().numpy()
    decoded = decoded.detach().numpy()

    # Select first sequence for comparison
    original_sequence = test_data_in[0]
    decoded_sequence = decoded[0]

    # Plotting
    plt.figure(figsize=(12, 6))

    # Plot original sequence
    plt.subplot(1, 2, 1)
    plt.plot(original_sequence)
    plt.title("Original Sequence")

    # Plot decoded sequence
    plt.subplot(1, 2, 2)
    plt.plot(decoded_sequence)
    plt.title("Decoded Sequence")

    plt.show()

def plot_features(test_data, generated_data, column_names, input_dim):
    ''' 
    Plot each feature of the first sequence for both test and generated data

    Parameters:
        test_data (numpy array): The original data
        generated_data (numpy array): The data produced by the model
        column_names (list of str): The names of the features
        input_dim (int): The number of features in the time series data
    '''
    
    # Ensure test_data and generated_data are numpy arrays
    test_data = test_data if isinstance(test_data, np.ndarray) else test_data.values
    generated_data = generated_data if isinstance(generated_data, np.ndarray) else generated_data.values

    # Iterate over and plot each feature of the first sequence
    for i in range(input_dim):
        plt.figure(i)
        plt.plot(generated_data[:,i])
        plt.plot(test_data[:,i])
        plt.title(f'{column_names[i]}')  
        plt.xlabel('Time Step')
        plt.ylabel('Value')
    plt.show()

def determine_components(df):
    """
    Function to determine the number of principal components for a given DataFrame.

    Parameters:
    df (pandas.DataFrame): The DataFrame to perform PCA on.

    Returns:
    int: The number of principal components.
    """

    # Normalize the data
    scaler = StandardScaler()
    scaled_data = scaler.fit_transform(df)

    # Create a PCA instance
    pca = PCA()

    # Fit PCA to the data
    pca.fit(scaled_data)

    # Determine the number of components
    num_components = sum(pca.explained_variance_ratio_.cumsum() <= 0.95)
    print(num_components)
    return num_components


def plot_pca(data1, data2):
    """
    Apply PCA to two datasets and plot the results on the same graph.

    Args:
        data1 (numpy.ndarray): The first dataset.
        data2 (numpy.ndarray): The second dataset.

    Returns:
        None
    """

    # Perform PCA on the datasets
    pca = PCA(n_components=2)  # We are reducing the dimension to 2 for visualization
    transformed_data1 = pca.fit_transform(data1)
    transformed_data2 = pca.transform(data2)  # Apply the transformation learned from data1 to data2

    # Plot the results
    plt.figure(figsize=(10, 7))
    plt.scatter(transformed_data1[:, 0], transformed_data1[:, 1], color='blue', alpha=0.7, label='Test Data')
    plt.scatter(transformed_data2[:, 0], transformed_data2[:, 1], color='red', alpha=0.5, label='Generated Data')
    plt.legend()
    plt.xlabel('Principal Component 1')
    plt.ylabel('Principal Component 2')
    plt.title('PCA of Test Data and Generated Data')
    plt.show()


def plot_tsne(data1, data2):
    """
    Apply t-SNE to two datasets and plot the results on the same graph.

    Args:
        data1 (numpy.ndarray): The first dataset.
        data2 (numpy.ndarray): The second dataset.

    Returns:
        None
    """

    # Perform t-SNE on the datasets
    tsne = TSNE(n_components=2)  # We are reducing the dimension to 2 for visualization
    all_data = np.concatenate((data1, data2), axis=0)
    transformed_data = tsne.fit_transform(all_data)

    # Separate the transformed data
    transformed_data1 = transformed_data[:data1.shape[0]]
    transformed_data2 = transformed_data[data1.shape[0]:]

    # Plot the results
    plt.figure(figsize=(10, 7))
    plt.scatter(transformed_data1[:, 0], transformed_data1[:, 1], color='blue', alpha=0.7, label='Test Data')
    plt.scatter(transformed_data2[:, 0], transformed_data2[:, 1], color='red', alpha=0.5, label='Generated Data')
    plt.legend()
    plt.xlabel('t-SNE Component 1')
    plt.ylabel('t-SNE Component 2')
    plt.title('t-SNE of Test Data and Generated Data')
    plt.show()
