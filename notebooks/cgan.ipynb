{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import getpass\n",
    "import sys\n",
    "sys.path.insert(0, f'/home/{getpass.getuser()}/dowgan/dowgan')\n",
    "import Util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV path\n",
    "folder_name = \"data\"\n",
    "file_name = \"hungary_chickenpox.csv\"\n",
    "path = os.path.join('../', folder_name, file_name)\n",
    "#Load CSV into Dataframe\n",
    "df = pd.read_csv(path,sep=',')\n",
    "df = df.drop(columns = ['Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BUDAPEST</th>\n",
       "      <th>BARANYA</th>\n",
       "      <th>BACS</th>\n",
       "      <th>BEKES</th>\n",
       "      <th>BORSOD</th>\n",
       "      <th>CSONGRAD</th>\n",
       "      <th>FEJER</th>\n",
       "      <th>GYOR</th>\n",
       "      <th>HAJDU</th>\n",
       "      <th>HEVES</th>\n",
       "      <th>JASZ</th>\n",
       "      <th>KOMAROM</th>\n",
       "      <th>NOGRAD</th>\n",
       "      <th>PEST</th>\n",
       "      <th>SOMOGY</th>\n",
       "      <th>SZABOLCS</th>\n",
       "      <th>TOLNA</th>\n",
       "      <th>VAS</th>\n",
       "      <th>VESZPREM</th>\n",
       "      <th>ZALA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>168</td>\n",
       "      <td>79</td>\n",
       "      <td>30</td>\n",
       "      <td>173</td>\n",
       "      <td>169</td>\n",
       "      <td>42</td>\n",
       "      <td>136</td>\n",
       "      <td>120</td>\n",
       "      <td>162</td>\n",
       "      <td>36</td>\n",
       "      <td>130</td>\n",
       "      <td>57</td>\n",
       "      <td>2</td>\n",
       "      <td>178</td>\n",
       "      <td>66</td>\n",
       "      <td>64</td>\n",
       "      <td>11</td>\n",
       "      <td>29</td>\n",
       "      <td>87</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>157</td>\n",
       "      <td>60</td>\n",
       "      <td>30</td>\n",
       "      <td>92</td>\n",
       "      <td>200</td>\n",
       "      <td>53</td>\n",
       "      <td>51</td>\n",
       "      <td>70</td>\n",
       "      <td>84</td>\n",
       "      <td>28</td>\n",
       "      <td>80</td>\n",
       "      <td>50</td>\n",
       "      <td>29</td>\n",
       "      <td>141</td>\n",
       "      <td>48</td>\n",
       "      <td>29</td>\n",
       "      <td>58</td>\n",
       "      <td>53</td>\n",
       "      <td>68</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>96</td>\n",
       "      <td>44</td>\n",
       "      <td>31</td>\n",
       "      <td>86</td>\n",
       "      <td>93</td>\n",
       "      <td>30</td>\n",
       "      <td>93</td>\n",
       "      <td>84</td>\n",
       "      <td>191</td>\n",
       "      <td>51</td>\n",
       "      <td>64</td>\n",
       "      <td>46</td>\n",
       "      <td>4</td>\n",
       "      <td>157</td>\n",
       "      <td>33</td>\n",
       "      <td>33</td>\n",
       "      <td>24</td>\n",
       "      <td>18</td>\n",
       "      <td>62</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>163</td>\n",
       "      <td>49</td>\n",
       "      <td>43</td>\n",
       "      <td>126</td>\n",
       "      <td>46</td>\n",
       "      <td>39</td>\n",
       "      <td>52</td>\n",
       "      <td>114</td>\n",
       "      <td>107</td>\n",
       "      <td>42</td>\n",
       "      <td>63</td>\n",
       "      <td>54</td>\n",
       "      <td>14</td>\n",
       "      <td>107</td>\n",
       "      <td>66</td>\n",
       "      <td>50</td>\n",
       "      <td>25</td>\n",
       "      <td>21</td>\n",
       "      <td>43</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>122</td>\n",
       "      <td>78</td>\n",
       "      <td>53</td>\n",
       "      <td>87</td>\n",
       "      <td>103</td>\n",
       "      <td>34</td>\n",
       "      <td>95</td>\n",
       "      <td>131</td>\n",
       "      <td>172</td>\n",
       "      <td>40</td>\n",
       "      <td>61</td>\n",
       "      <td>49</td>\n",
       "      <td>11</td>\n",
       "      <td>124</td>\n",
       "      <td>63</td>\n",
       "      <td>56</td>\n",
       "      <td>7</td>\n",
       "      <td>47</td>\n",
       "      <td>85</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517</th>\n",
       "      <td>95</td>\n",
       "      <td>12</td>\n",
       "      <td>41</td>\n",
       "      <td>6</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>15</td>\n",
       "      <td>14</td>\n",
       "      <td>10</td>\n",
       "      <td>56</td>\n",
       "      <td>7</td>\n",
       "      <td>13</td>\n",
       "      <td>122</td>\n",
       "      <td>4</td>\n",
       "      <td>23</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>110</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>518</th>\n",
       "      <td>43</td>\n",
       "      <td>39</td>\n",
       "      <td>31</td>\n",
       "      <td>10</td>\n",
       "      <td>34</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>30</td>\n",
       "      <td>25</td>\n",
       "      <td>19</td>\n",
       "      <td>34</td>\n",
       "      <td>20</td>\n",
       "      <td>18</td>\n",
       "      <td>70</td>\n",
       "      <td>36</td>\n",
       "      <td>5</td>\n",
       "      <td>23</td>\n",
       "      <td>22</td>\n",
       "      <td>63</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>519</th>\n",
       "      <td>35</td>\n",
       "      <td>7</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>30</td>\n",
       "      <td>36</td>\n",
       "      <td>4</td>\n",
       "      <td>72</td>\n",
       "      <td>5</td>\n",
       "      <td>21</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>520</th>\n",
       "      <td>30</td>\n",
       "      <td>23</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>17</td>\n",
       "      <td>27</td>\n",
       "      <td>17</td>\n",
       "      <td>21</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>83</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>521</th>\n",
       "      <td>259</td>\n",
       "      <td>42</td>\n",
       "      <td>49</td>\n",
       "      <td>32</td>\n",
       "      <td>38</td>\n",
       "      <td>15</td>\n",
       "      <td>11</td>\n",
       "      <td>98</td>\n",
       "      <td>61</td>\n",
       "      <td>38</td>\n",
       "      <td>112</td>\n",
       "      <td>61</td>\n",
       "      <td>53</td>\n",
       "      <td>256</td>\n",
       "      <td>45</td>\n",
       "      <td>39</td>\n",
       "      <td>27</td>\n",
       "      <td>11</td>\n",
       "      <td>103</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>522 rows Ã— 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     BUDAPEST  BARANYA  BACS  BEKES  BORSOD  CSONGRAD  FEJER  GYOR  HAJDU  \\\n",
       "0         168       79    30    173     169        42    136   120    162   \n",
       "1         157       60    30     92     200        53     51    70     84   \n",
       "2          96       44    31     86      93        30     93    84    191   \n",
       "3         163       49    43    126      46        39     52   114    107   \n",
       "4         122       78    53     87     103        34     95   131    172   \n",
       "..        ...      ...   ...    ...     ...       ...    ...   ...    ...   \n",
       "517        95       12    41      6      39         0     16    15     14   \n",
       "518        43       39    31     10      34         3      2    30     25   \n",
       "519        35        7    15      0       0         0      7     7      4   \n",
       "520        30       23     8      0      11         4      1     9     10   \n",
       "521       259       42    49     32      38        15     11    98     61   \n",
       "\n",
       "     HEVES  JASZ  KOMAROM  NOGRAD  PEST  SOMOGY  SZABOLCS  TOLNA  VAS  \\\n",
       "0       36   130       57       2   178      66        64     11   29   \n",
       "1       28    80       50      29   141      48        29     58   53   \n",
       "2       51    64       46       4   157      33        33     24   18   \n",
       "3       42    63       54      14   107      66        50     25   21   \n",
       "4       40    61       49      11   124      63        56      7   47   \n",
       "..     ...   ...      ...     ...   ...     ...       ...    ...  ...   \n",
       "517     10    56        7      13   122       4        23      4   11   \n",
       "518     19    34       20      18    70      36         5     23   22   \n",
       "519      2    30       36       4    72       5        21     14    0   \n",
       "520     17    27       17      21    12       5        17      1    1   \n",
       "521     38   112       61      53   256      45        39     27   11   \n",
       "\n",
       "     VESZPREM  ZALA  \n",
       "0          87    68  \n",
       "1          68    26  \n",
       "2          62    44  \n",
       "3          43    31  \n",
       "4          85    60  \n",
       "..        ...   ...  \n",
       "517       110    10  \n",
       "518        63     9  \n",
       "519        17    10  \n",
       "520        83     2  \n",
       "521       103    25  \n",
       "\n",
       "[522 rows x 20 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = nn.Embedding(df.shape[1],100)\n",
    "embeddings.weight.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, embeddings, nc=3, ndf=64):\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        self.embeddings = embeddings\n",
    "        self.label_to_image = nn.Linear(100,32*32*3)\n",
    "        self.conv1 = nn.Conv2d(nc * 2, nc, 1, 1, 0, bias=False)\n",
    "        \n",
    "        self.main = nn.Sequential(\n",
    "            # input is (nc) x 64 x 64\n",
    "            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf) x 32 x 32\n",
    "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*2) x 16 x 16\n",
    "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*4) x 8 x 8\n",
    "            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*8) x 4 x 4\n",
    "            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, label_embed):\n",
    "        \n",
    "        label_embed = self.embeddings(label_embed)\n",
    "\n",
    "        label_map = self.label_to_image(label_embed)\n",
    "        label_map = label_map.view(-1,3,32,32)\n",
    "\n",
    "        x = torch.cat([x,label_map], dim=1)\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        output = self.main(out)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, embeddings, nc=3, nz=100, ngf=64):\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        self.embeddings = embeddings\n",
    "        self.linear = nn.Linear(200,100)\n",
    "        \n",
    "        self.main = nn.Sequential(\n",
    "            # input is Z, going into a convolution\n",
    "            nn.ConvTranspose2d(     nz, ngf * 8, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 8),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*8) x 4 x 4\n",
    "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*4) x 8 x 8\n",
    "            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 2),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*2) x 16 x 16\n",
    "            nn.ConvTranspose2d(ngf * 2,     ngf, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf) x 32 x 32\n",
    "            nn.ConvTranspose2d(    ngf,      nc, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "            # state size. (nc) x 64 x 64\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, label_embed):\n",
    "        label_embed = self.embeddings(label_embed)\n",
    "\n",
    "        x = x.view(-1,100)\n",
    "        x = torch.cat([x,label_embed], dim=1)\n",
    "\n",
    "        x = self.linear(x)\n",
    "        x = x.unsqueeze(2).unsqueeze(3)\n",
    "\n",
    "        output = self.main(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random state\n",
    "torch.manual_seed(111)\n",
    "# Select # of data points from data set to train GAN\n",
    "num_data = 450\n",
    "# Specify Validation Data\n",
    "val_data = 50\n",
    "# Specify dimensionality of dataframe\n",
    "df_dim = len(df.columns)\n",
    "# learning rate\n",
    "lr = 0.002\n",
    "# number of epoch\n",
    "epochs = 1000\n",
    "# Discriminator and Generator dropout fro data standardization\n",
    "drop_out=0.3\n",
    "# Define batch size for the data loader\n",
    "batch_size = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Tensors must have same number of dimensions: got 2 and 4",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 42\u001b[0m\n\u001b[1;32m     36\u001b[0m label_embed \u001b[39m=\u001b[39m label\u001b[39m.\u001b[39mto(device)\u001b[39m.\u001b[39mlong()\n\u001b[1;32m     38\u001b[0m \u001b[39m\u001b[39m\u001b[39m'''\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[39m    Update D network: maximize log(D(x)) + log(1 - D(G(z)))\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[39m'''\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m D_real_result \u001b[39m=\u001b[39m netD(input_sequence, label_embed)\n\u001b[1;32m     43\u001b[0m D_real_loss \u001b[39m=\u001b[39m criterion(D_real_result\u001b[39m.\u001b[39mview(batch_size,\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m), real_label)\n\u001b[1;32m     45\u001b[0m G_result \u001b[39m=\u001b[39m netG(fixed_noise,label_embed)\n",
      "File \u001b[0;32m~/miniconda3/envs/dowgan/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[6], line 37\u001b[0m, in \u001b[0;36mDiscriminator.forward\u001b[0;34m(self, x, label_embed)\u001b[0m\n\u001b[1;32m     34\u001b[0m label_map \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlabel_to_image(label_embed)\n\u001b[1;32m     35\u001b[0m label_map \u001b[39m=\u001b[39m label_map\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m,\u001b[39m3\u001b[39m,\u001b[39m32\u001b[39m,\u001b[39m32\u001b[39m)\n\u001b[0;32m---> 37\u001b[0m x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mcat([x,label_map], dim\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[1;32m     39\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv1(x)\n\u001b[1;32m     40\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmain(out)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Tensors must have same number of dimensions: got 2 and 4"
     ]
    }
   ],
   "source": [
    "# These create the arrays and then turns them into tensors for the train loader \n",
    "arrays = Util.create_arrays(df[:num_data],num_data)\n",
    "train_set = Util.create_tensors(arrays)\n",
    "\n",
    "train_data_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size)\n",
    "\n",
    "\n",
    "embeddings = nn.Embedding(10,100)\n",
    "embeddings.weight.requires_grad = False\n",
    "embeddings(torch.LongTensor())\n",
    "\n",
    "netD = Discriminator(embeddings)\n",
    "netG = Generator(embeddings)\n",
    "\n",
    "\n",
    "optimizerD = optim.Adam(netD.parameters(),lr=0.0002,betas=(0.5, 0.999))\n",
    "optimizerG = optim.Adam(netG.parameters(),lr=0.0002,betas=(0.5, 0.999))\n",
    "\n",
    "netD.train()\n",
    "netG.train()\n",
    "\n",
    "nz = 100\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "real_label = torch.ones([batch_size,1], dtype=torch.float).to(device)\n",
    "fake_label = torch.zeros([batch_size,1], dtype=torch.float).to(device)\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i, (input_sequence, label) in enumerate(train_data_loader):\n",
    "        \n",
    "        fixed_noise = torch.randn(batch_size, nz, 1, 1, device=device)\n",
    "\n",
    "        input_sequence = input_sequence.to(device)\n",
    "        label_embed = label.to(device).long()\n",
    "        \n",
    "        '''\n",
    "            Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "        '''\n",
    "\n",
    "        D_real_result = netD(input_sequence, label_embed)\n",
    "        D_real_loss = criterion(D_real_result.view(batch_size,-1), real_label)\n",
    "\n",
    "        G_result = netG(fixed_noise,label_embed)\n",
    "\n",
    "        D_fake_result = netD(G_result,label_embed)\n",
    "\n",
    "        D_fake_loss = criterion(D_fake_result.view(batch_size,-1), fake_label)\n",
    "\n",
    "        # Back propagation\n",
    "        D_train_loss = (D_real_loss + D_fake_loss) / 2\n",
    "\n",
    "        netD.zero_grad()\n",
    "        D_train_loss.backward()\n",
    "        optimizerD.step()\n",
    "\n",
    "        '''\n",
    "            Update G network: maximize log(D(G(z)))\n",
    "        '''\n",
    "        new_label = torch.LongTensor(batch_size,10).random_(0, 10).to(device)\n",
    "        new_embed = new_label[:,0].view(-1)\n",
    "\n",
    "        G_result = netG(fixed_noise, new_embed)\n",
    "\n",
    "        D_fake_result = netD(G_result, new_embed)\n",
    "        G_train_loss = criterion(D_fake_result.view(batch_size,-1), real_label)\n",
    "\n",
    "\n",
    "        # Back propagation\n",
    "        netD.zero_grad()\n",
    "        netG.zero_grad()\n",
    "        G_train_loss.backward()\n",
    "        optimizerG.step()\n",
    "        \n",
    "        print(\"D_loss:%f\\tG_loss:%f\" % (D_train_loss,G_train_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dowgan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
