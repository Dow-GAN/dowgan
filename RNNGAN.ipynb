{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d0f6ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "import Util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f3169a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('hungary_chickenpox.csv',sep=',')\n",
    "df = df.drop(columns = ['Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2b76dd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BUDAPEST</th>\n",
       "      <th>BARANYA</th>\n",
       "      <th>BACS</th>\n",
       "      <th>BEKES</th>\n",
       "      <th>BORSOD</th>\n",
       "      <th>CSONGRAD</th>\n",
       "      <th>FEJER</th>\n",
       "      <th>GYOR</th>\n",
       "      <th>HAJDU</th>\n",
       "      <th>HEVES</th>\n",
       "      <th>JASZ</th>\n",
       "      <th>KOMAROM</th>\n",
       "      <th>NOGRAD</th>\n",
       "      <th>PEST</th>\n",
       "      <th>SOMOGY</th>\n",
       "      <th>SZABOLCS</th>\n",
       "      <th>TOLNA</th>\n",
       "      <th>VAS</th>\n",
       "      <th>VESZPREM</th>\n",
       "      <th>ZALA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>168</td>\n",
       "      <td>79</td>\n",
       "      <td>30</td>\n",
       "      <td>173</td>\n",
       "      <td>169</td>\n",
       "      <td>42</td>\n",
       "      <td>136</td>\n",
       "      <td>120</td>\n",
       "      <td>162</td>\n",
       "      <td>36</td>\n",
       "      <td>130</td>\n",
       "      <td>57</td>\n",
       "      <td>2</td>\n",
       "      <td>178</td>\n",
       "      <td>66</td>\n",
       "      <td>64</td>\n",
       "      <td>11</td>\n",
       "      <td>29</td>\n",
       "      <td>87</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>157</td>\n",
       "      <td>60</td>\n",
       "      <td>30</td>\n",
       "      <td>92</td>\n",
       "      <td>200</td>\n",
       "      <td>53</td>\n",
       "      <td>51</td>\n",
       "      <td>70</td>\n",
       "      <td>84</td>\n",
       "      <td>28</td>\n",
       "      <td>80</td>\n",
       "      <td>50</td>\n",
       "      <td>29</td>\n",
       "      <td>141</td>\n",
       "      <td>48</td>\n",
       "      <td>29</td>\n",
       "      <td>58</td>\n",
       "      <td>53</td>\n",
       "      <td>68</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>96</td>\n",
       "      <td>44</td>\n",
       "      <td>31</td>\n",
       "      <td>86</td>\n",
       "      <td>93</td>\n",
       "      <td>30</td>\n",
       "      <td>93</td>\n",
       "      <td>84</td>\n",
       "      <td>191</td>\n",
       "      <td>51</td>\n",
       "      <td>64</td>\n",
       "      <td>46</td>\n",
       "      <td>4</td>\n",
       "      <td>157</td>\n",
       "      <td>33</td>\n",
       "      <td>33</td>\n",
       "      <td>24</td>\n",
       "      <td>18</td>\n",
       "      <td>62</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>163</td>\n",
       "      <td>49</td>\n",
       "      <td>43</td>\n",
       "      <td>126</td>\n",
       "      <td>46</td>\n",
       "      <td>39</td>\n",
       "      <td>52</td>\n",
       "      <td>114</td>\n",
       "      <td>107</td>\n",
       "      <td>42</td>\n",
       "      <td>63</td>\n",
       "      <td>54</td>\n",
       "      <td>14</td>\n",
       "      <td>107</td>\n",
       "      <td>66</td>\n",
       "      <td>50</td>\n",
       "      <td>25</td>\n",
       "      <td>21</td>\n",
       "      <td>43</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>122</td>\n",
       "      <td>78</td>\n",
       "      <td>53</td>\n",
       "      <td>87</td>\n",
       "      <td>103</td>\n",
       "      <td>34</td>\n",
       "      <td>95</td>\n",
       "      <td>131</td>\n",
       "      <td>172</td>\n",
       "      <td>40</td>\n",
       "      <td>61</td>\n",
       "      <td>49</td>\n",
       "      <td>11</td>\n",
       "      <td>124</td>\n",
       "      <td>63</td>\n",
       "      <td>56</td>\n",
       "      <td>7</td>\n",
       "      <td>47</td>\n",
       "      <td>85</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517</th>\n",
       "      <td>95</td>\n",
       "      <td>12</td>\n",
       "      <td>41</td>\n",
       "      <td>6</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>15</td>\n",
       "      <td>14</td>\n",
       "      <td>10</td>\n",
       "      <td>56</td>\n",
       "      <td>7</td>\n",
       "      <td>13</td>\n",
       "      <td>122</td>\n",
       "      <td>4</td>\n",
       "      <td>23</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>110</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>518</th>\n",
       "      <td>43</td>\n",
       "      <td>39</td>\n",
       "      <td>31</td>\n",
       "      <td>10</td>\n",
       "      <td>34</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>30</td>\n",
       "      <td>25</td>\n",
       "      <td>19</td>\n",
       "      <td>34</td>\n",
       "      <td>20</td>\n",
       "      <td>18</td>\n",
       "      <td>70</td>\n",
       "      <td>36</td>\n",
       "      <td>5</td>\n",
       "      <td>23</td>\n",
       "      <td>22</td>\n",
       "      <td>63</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>519</th>\n",
       "      <td>35</td>\n",
       "      <td>7</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>30</td>\n",
       "      <td>36</td>\n",
       "      <td>4</td>\n",
       "      <td>72</td>\n",
       "      <td>5</td>\n",
       "      <td>21</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>520</th>\n",
       "      <td>30</td>\n",
       "      <td>23</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>17</td>\n",
       "      <td>27</td>\n",
       "      <td>17</td>\n",
       "      <td>21</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>83</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>521</th>\n",
       "      <td>259</td>\n",
       "      <td>42</td>\n",
       "      <td>49</td>\n",
       "      <td>32</td>\n",
       "      <td>38</td>\n",
       "      <td>15</td>\n",
       "      <td>11</td>\n",
       "      <td>98</td>\n",
       "      <td>61</td>\n",
       "      <td>38</td>\n",
       "      <td>112</td>\n",
       "      <td>61</td>\n",
       "      <td>53</td>\n",
       "      <td>256</td>\n",
       "      <td>45</td>\n",
       "      <td>39</td>\n",
       "      <td>27</td>\n",
       "      <td>11</td>\n",
       "      <td>103</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>522 rows Ã— 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     BUDAPEST  BARANYA  BACS  BEKES  BORSOD  CSONGRAD  FEJER  GYOR  HAJDU  \\\n",
       "0         168       79    30    173     169        42    136   120    162   \n",
       "1         157       60    30     92     200        53     51    70     84   \n",
       "2          96       44    31     86      93        30     93    84    191   \n",
       "3         163       49    43    126      46        39     52   114    107   \n",
       "4         122       78    53     87     103        34     95   131    172   \n",
       "..        ...      ...   ...    ...     ...       ...    ...   ...    ...   \n",
       "517        95       12    41      6      39         0     16    15     14   \n",
       "518        43       39    31     10      34         3      2    30     25   \n",
       "519        35        7    15      0       0         0      7     7      4   \n",
       "520        30       23     8      0      11         4      1     9     10   \n",
       "521       259       42    49     32      38        15     11    98     61   \n",
       "\n",
       "     HEVES  JASZ  KOMAROM  NOGRAD  PEST  SOMOGY  SZABOLCS  TOLNA  VAS  \\\n",
       "0       36   130       57       2   178      66        64     11   29   \n",
       "1       28    80       50      29   141      48        29     58   53   \n",
       "2       51    64       46       4   157      33        33     24   18   \n",
       "3       42    63       54      14   107      66        50     25   21   \n",
       "4       40    61       49      11   124      63        56      7   47   \n",
       "..     ...   ...      ...     ...   ...     ...       ...    ...  ...   \n",
       "517     10    56        7      13   122       4        23      4   11   \n",
       "518     19    34       20      18    70      36         5     23   22   \n",
       "519      2    30       36       4    72       5        21     14    0   \n",
       "520     17    27       17      21    12       5        17      1    1   \n",
       "521     38   112       61      53   256      45        39     27   11   \n",
       "\n",
       "     VESZPREM  ZALA  \n",
       "0          87    68  \n",
       "1          68    26  \n",
       "2          62    44  \n",
       "3          43    31  \n",
       "4          85    60  \n",
       "..        ...   ...  \n",
       "517       110    10  \n",
       "518        63     9  \n",
       "519        17    10  \n",
       "520        83     2  \n",
       "521       103    25  \n",
       "\n",
       "[522 rows x 20 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ab6d31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_data = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68b4a7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = df.iloc[:num_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce5edf1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8aa57e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df_1.to_numpy()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22dbd29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = len(df.columns)\n",
    "output_size= len(df.columns)\n",
    "hidden_size = len(df.columns)\n",
    "num_layers = 3\n",
    "z_dim = 20\n",
    "batch_size = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "79900262",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert numpy array to PyTorch tensor\n",
    "tensor = torch.tensor(data, dtype=torch.float32)\n",
    "\n",
    "# Create TimeSeriesDataset object\n",
    "dataset = TensorDataset(tensor)\n",
    "\n",
    "# Create DataLoader for batch processing\n",
    "dataloader = DataLoader(dataset, batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c17068",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8dabe45",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        return torch.tensor(sample, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "efd81d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Generator(nn.Module):\n",
    "#     def __init__(self, input_size, hidden_size, output_size, num_layers):\n",
    "#         super().__init__()\n",
    "#         self.rnn = nn.GRU(input_size, hidden_size, num_layers)\n",
    "#         self.fc = nn.Linear(hidden_size, output_size)\n",
    "#         self.dropout = nn.Dropout(0.3)\n",
    "#         self.relu = nn.ReLU()\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         x, _ = self.rnn(x)\n",
    "#         x = self.dropout(x)\n",
    "#         x = self.fc(x)\n",
    "#         x = self.relu(x)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ac372f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.GRU(input_size, hidden_size, num_layers)\n",
    "        self.fc1 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x, _ = self.rnn(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac12066d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x, _ = self.rnn(x)\n",
    "        x = self.fc(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3dd3200e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_gan(generator, discriminator, dataloader, batch_size, num_epochs, lr_g=0.001, lr_d=0.001, critic_iters=3):\n",
    "#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#     generator.to(device)\n",
    "#     discriminator.to(device)\n",
    "#     optimizer_g = optim.Adam(generator.parameters(), lr=lr_g, betas=(0.5, 0.999))\n",
    "#     optimizer_d = optim.Adam(discriminator.parameters(), lr=lr_d, betas=(0.5, 0.999))\n",
    "\n",
    "#     for epoch in range(num_epochs):\n",
    "#         for i, real_samples in enumerate(dataloader):\n",
    "#             real_samples = torch.stack(real_samples, dim=0).to(device)\n",
    "\n",
    "#             # Train discriminator\n",
    "#             for critic_iter in range(critic_iters):\n",
    "#                 noise = torch.randn(batch_size, 1, 20, device=device)\n",
    "#                 fake_samples = generator(noise)\n",
    "#                 fake_output = discriminator(fake_samples.detach())\n",
    "#                 real_output = discriminator(real_samples)\n",
    "\n",
    "#                 # Wasserstein loss\n",
    "#                 loss_d = -torch.mean(real_output) + torch.mean(fake_output)\n",
    "#                 gradient_penalty = calc_gradient_penalty(discriminator, real_samples, fake_samples.detach(), device=device)\n",
    "#                 loss_d += gradient_penalty\n",
    "\n",
    "#                 optimizer_d.zero_grad()\n",
    "#                 loss_d.backward()\n",
    "#                 optimizer_d.step()\n",
    "\n",
    "#             # Train generator\n",
    "#             noise = torch.randn(batch_size, 1, 20, device=device)\n",
    "#             fake_samples = generator(noise)\n",
    "#             fake_output = discriminator(fake_samples)\n",
    "#             loss_g = -torch.mean(fake_output)\n",
    "\n",
    "#             optimizer_g.zero_grad()\n",
    "#             loss_g.backward()\n",
    "#             optimizer_g.step()\n",
    "\n",
    "#             if i % 100 == 0:\n",
    "#                 print(f\"Epoch {epoch+1}/{num_epochs} | Discriminator Loss: {loss_d.item():.4f} | Generator Loss: {loss_g.item():.4f}\")\n",
    "\n",
    "# def calc_gradient_penalty(discriminator, real_samples, fake_samples, device):\n",
    "#     alpha = torch.rand(real_samples.size(0), 1, 1, 1, device=device)\n",
    "#     interpolated = (alpha * real_samples + (1 - alpha) * fake_samples).requires_grad_(True)\n",
    "#     prob_interpolated = discriminator(interpolated)\n",
    "#     gradients = torch.autograd.grad(outputs=prob_interpolated, inputs=interpolated,\n",
    "#                                     grad_outputs=torch.ones(prob_interpolated.size(), device=device),\n",
    "#                                     create_graph=True, retain_graph=True)[0]\n",
    "#     gradients = gradients.view(real_samples.size(0), -1)\n",
    "#     gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "#     return gradient_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ceddb489",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gan(generator, discriminator, dataloader,batch_size, num_epochs, lr_g=0.001, lr_d=0.001):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    generator.to(device)\n",
    "    discriminator.to(device)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer_g = optim.Adam(generator.parameters(), lr=lr_g, betas=(0.5, 0.999))\n",
    "    optimizer_d = optim.Adam(discriminator.parameters(), lr=lr_d, betas=(0.5, 0.999))\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        for i, real_samples in enumerate(dataloader):\n",
    "            \n",
    "            \n",
    "            # Train discriminator\n",
    "            optimizer_d.zero_grad()\n",
    "            \n",
    "            # Train on real samples\n",
    "            real_samples = torch.stack(real_samples, dim=0).to(device)\n",
    "            real_labels = torch.ones(batch_size, device=device)\n",
    "            real_output = discriminator(real_samples)\n",
    "            loss_d_real = criterion(real_output, real_labels)\n",
    "            \n",
    "            # Train on fake samples\n",
    "            noise = torch.randn(batch_size, 1, 20, device=device)\n",
    "            fake_samples = generator(noise)\n",
    "            fake_labels = torch.zeros(batch_size, device=device)\n",
    "            fake_output = discriminator(fake_samples.detach())\n",
    "            loss_d_fake = criterion(fake_output, fake_labels)\n",
    "            \n",
    "            loss_d = loss_d_real + loss_d_fake\n",
    "            loss_d.backward()\n",
    "            optimizer_d.step()\n",
    "            \n",
    "            # Train generator\n",
    "            optimizer_g.zero_grad()\n",
    "            \n",
    "            noise = torch.randn(batch_size, 1, 20, device=device)\n",
    "            fake_samples = generator(noise)\n",
    "            fake_labels = torch.ones(batch_size, device=device)\n",
    "            fake_output = discriminator(fake_samples)\n",
    "            loss_g = criterion(fake_output, fake_labels)\n",
    "            \n",
    "            loss_g.backward()\n",
    "            optimizer_g.step()\n",
    "            \n",
    "            if i % 100 == 0:\n",
    "                print(f\"Epoch {epoch+1}/{num_epochs} | Discriminator Loss: {loss_d.item():.4f} | Generator Loss: {loss_g.item():.4f}\")\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d775bb78",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator(input_size,output_size, hidden_size, num_layers)\n",
    "discriminator = Discriminator(input_size, hidden_size, num_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c2ade938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 | Discriminator Loss: 0.8319 | Generator Loss: 0.6931\n",
      "Epoch 2/100 | Discriminator Loss: 0.8665 | Generator Loss: 0.6931\n",
      "Epoch 3/100 | Discriminator Loss: 0.9011 | Generator Loss: 0.6931\n",
      "Epoch 4/100 | Discriminator Loss: 0.9704 | Generator Loss: 0.6931\n",
      "Epoch 5/100 | Discriminator Loss: 0.9014 | Generator Loss: 0.6931\n",
      "Epoch 6/100 | Discriminator Loss: 0.9012 | Generator Loss: 0.6931\n",
      "Epoch 7/100 | Discriminator Loss: 0.8318 | Generator Loss: 0.6931\n",
      "Epoch 8/100 | Discriminator Loss: 0.7626 | Generator Loss: 0.6931\n",
      "Epoch 9/100 | Discriminator Loss: 0.9012 | Generator Loss: 0.6931\n",
      "Epoch 10/100 | Discriminator Loss: 0.7972 | Generator Loss: 0.6931\n",
      "Epoch 11/100 | Discriminator Loss: 0.9011 | Generator Loss: 0.6931\n",
      "Epoch 12/100 | Discriminator Loss: 0.9011 | Generator Loss: 0.6931\n",
      "Epoch 13/100 | Discriminator Loss: 0.9705 | Generator Loss: 0.6931\n",
      "Epoch 14/100 | Discriminator Loss: 0.9012 | Generator Loss: 0.6931\n",
      "Epoch 15/100 | Discriminator Loss: 0.9705 | Generator Loss: 0.6931\n",
      "Epoch 16/100 | Discriminator Loss: 0.9704 | Generator Loss: 0.6931\n",
      "Epoch 17/100 | Discriminator Loss: 0.8690 | Generator Loss: 0.6931\n",
      "Epoch 18/100 | Discriminator Loss: 1.0051 | Generator Loss: 0.6931\n",
      "Epoch 19/100 | Discriminator Loss: 0.9358 | Generator Loss: 0.6931\n",
      "Epoch 20/100 | Discriminator Loss: 0.8665 | Generator Loss: 0.6931\n",
      "Epoch 21/100 | Discriminator Loss: 0.8318 | Generator Loss: 0.6931\n",
      "Epoch 22/100 | Discriminator Loss: 0.9012 | Generator Loss: 0.6931\n",
      "Epoch 23/100 | Discriminator Loss: 0.9704 | Generator Loss: 0.6931\n",
      "Epoch 24/100 | Discriminator Loss: 0.9012 | Generator Loss: 0.6931\n",
      "Epoch 25/100 | Discriminator Loss: 0.8666 | Generator Loss: 0.6931\n",
      "Epoch 26/100 | Discriminator Loss: 0.8330 | Generator Loss: 0.6931\n",
      "Epoch 27/100 | Discriminator Loss: 0.8665 | Generator Loss: 0.6931\n",
      "Epoch 28/100 | Discriminator Loss: 0.8665 | Generator Loss: 0.6931\n",
      "Epoch 29/100 | Discriminator Loss: 0.9704 | Generator Loss: 0.6931\n",
      "Epoch 30/100 | Discriminator Loss: 0.9705 | Generator Loss: 0.6931\n",
      "Epoch 31/100 | Discriminator Loss: 0.9704 | Generator Loss: 0.6931\n",
      "Epoch 32/100 | Discriminator Loss: 0.8854 | Generator Loss: 0.6698\n",
      "Epoch 33/100 | Discriminator Loss: 0.9358 | Generator Loss: 0.6931\n",
      "Epoch 34/100 | Discriminator Loss: 0.8664 | Generator Loss: 0.6931\n",
      "Epoch 35/100 | Discriminator Loss: 0.9358 | Generator Loss: 0.6931\n",
      "Epoch 36/100 | Discriminator Loss: 0.9358 | Generator Loss: 0.6931\n",
      "Epoch 37/100 | Discriminator Loss: 1.0054 | Generator Loss: 0.6931\n",
      "Epoch 38/100 | Discriminator Loss: 0.8664 | Generator Loss: 0.6931\n",
      "Epoch 39/100 | Discriminator Loss: 0.8664 | Generator Loss: 0.6931\n",
      "Epoch 40/100 | Discriminator Loss: 1.0690 | Generator Loss: 0.6931\n",
      "Epoch 41/100 | Discriminator Loss: 0.9704 | Generator Loss: 0.6931\n",
      "Epoch 42/100 | Discriminator Loss: 0.9358 | Generator Loss: 0.6931\n",
      "Epoch 43/100 | Discriminator Loss: 0.9705 | Generator Loss: 0.6931\n",
      "Epoch 44/100 | Discriminator Loss: 0.7972 | Generator Loss: 0.6931\n",
      "Epoch 45/100 | Discriminator Loss: 0.9011 | Generator Loss: 0.6931\n",
      "Epoch 46/100 | Discriminator Loss: 0.9857 | Generator Loss: 0.6931\n",
      "Epoch 47/100 | Discriminator Loss: 0.9012 | Generator Loss: 0.6931\n",
      "Epoch 48/100 | Discriminator Loss: 0.9359 | Generator Loss: 0.6931\n",
      "Epoch 49/100 | Discriminator Loss: 0.8665 | Generator Loss: 0.6931\n",
      "Epoch 50/100 | Discriminator Loss: 0.8382 | Generator Loss: 0.6931\n",
      "Epoch 51/100 | Discriminator Loss: 0.8742 | Generator Loss: 0.6931\n",
      "Epoch 52/100 | Discriminator Loss: 0.9359 | Generator Loss: 0.6931\n",
      "Epoch 53/100 | Discriminator Loss: 0.8319 | Generator Loss: 0.6931\n",
      "Epoch 54/100 | Discriminator Loss: 0.9011 | Generator Loss: 0.6931\n",
      "Epoch 55/100 | Discriminator Loss: 0.9023 | Generator Loss: 0.6931\n",
      "Epoch 56/100 | Discriminator Loss: 0.9704 | Generator Loss: 0.6931\n",
      "Epoch 57/100 | Discriminator Loss: 0.9013 | Generator Loss: 0.6931\n",
      "Epoch 58/100 | Discriminator Loss: 0.9012 | Generator Loss: 0.6931\n",
      "Epoch 59/100 | Discriminator Loss: 0.9363 | Generator Loss: 0.6931\n",
      "Epoch 60/100 | Discriminator Loss: 0.9358 | Generator Loss: 0.6931\n",
      "Epoch 61/100 | Discriminator Loss: 0.9051 | Generator Loss: 0.6931\n",
      "Epoch 62/100 | Discriminator Loss: 0.8665 | Generator Loss: 0.6931\n",
      "Epoch 63/100 | Discriminator Loss: 0.9705 | Generator Loss: 0.6931\n",
      "Epoch 64/100 | Discriminator Loss: 0.9704 | Generator Loss: 0.6931\n",
      "Epoch 65/100 | Discriminator Loss: 0.9358 | Generator Loss: 0.6931\n",
      "Epoch 66/100 | Discriminator Loss: 0.8318 | Generator Loss: 0.6931\n",
      "Epoch 67/100 | Discriminator Loss: 0.9358 | Generator Loss: 0.6931\n",
      "Epoch 68/100 | Discriminator Loss: 1.0051 | Generator Loss: 0.6931\n",
      "Epoch 69/100 | Discriminator Loss: 0.9011 | Generator Loss: 0.6931\n",
      "Epoch 70/100 | Discriminator Loss: 0.9704 | Generator Loss: 0.6931\n",
      "Epoch 71/100 | Discriminator Loss: 0.7278 | Generator Loss: 0.6931\n",
      "Epoch 72/100 | Discriminator Loss: 0.9397 | Generator Loss: 0.6931\n",
      "Epoch 73/100 | Discriminator Loss: 0.9361 | Generator Loss: 0.6931\n",
      "Epoch 74/100 | Discriminator Loss: 0.9358 | Generator Loss: 0.6931\n",
      "Epoch 75/100 | Discriminator Loss: 0.9011 | Generator Loss: 0.6931\n",
      "Epoch 76/100 | Discriminator Loss: 0.9202 | Generator Loss: 0.6697\n",
      "Epoch 77/100 | Discriminator Loss: 0.8764 | Generator Loss: 0.6931\n",
      "Epoch 78/100 | Discriminator Loss: 0.9011 | Generator Loss: 0.6931\n",
      "Epoch 79/100 | Discriminator Loss: 0.8796 | Generator Loss: 0.6931\n",
      "Epoch 80/100 | Discriminator Loss: 0.9862 | Generator Loss: 0.6690\n",
      "Epoch 81/100 | Discriminator Loss: 0.8319 | Generator Loss: 0.6931\n",
      "Epoch 82/100 | Discriminator Loss: 0.9011 | Generator Loss: 0.6931\n",
      "Epoch 83/100 | Discriminator Loss: 1.0744 | Generator Loss: 0.6931\n",
      "Epoch 84/100 | Discriminator Loss: 0.9704 | Generator Loss: 0.6931\n",
      "Epoch 85/100 | Discriminator Loss: 0.8318 | Generator Loss: 0.6931\n",
      "Epoch 86/100 | Discriminator Loss: 0.8617 | Generator Loss: 0.6931\n",
      "Epoch 87/100 | Discriminator Loss: 1.0051 | Generator Loss: 0.6931\n",
      "Epoch 88/100 | Discriminator Loss: 1.0422 | Generator Loss: 0.6931\n",
      "Epoch 89/100 | Discriminator Loss: 0.9071 | Generator Loss: 0.6931\n",
      "Epoch 90/100 | Discriminator Loss: 0.8790 | Generator Loss: 0.6931\n",
      "Epoch 91/100 | Discriminator Loss: 1.0077 | Generator Loss: 0.6931\n",
      "Epoch 92/100 | Discriminator Loss: 0.8665 | Generator Loss: 0.6931\n",
      "Epoch 93/100 | Discriminator Loss: 0.9011 | Generator Loss: 0.6931\n",
      "Epoch 94/100 | Discriminator Loss: 0.9529 | Generator Loss: 0.6931\n",
      "Epoch 95/100 | Discriminator Loss: 0.8665 | Generator Loss: 0.6931\n",
      "Epoch 96/100 | Discriminator Loss: 0.9912 | Generator Loss: 0.6931\n",
      "Epoch 97/100 | Discriminator Loss: 0.8318 | Generator Loss: 0.6931\n",
      "Epoch 98/100 | Discriminator Loss: 0.9301 | Generator Loss: 0.6891\n",
      "Epoch 99/100 | Discriminator Loss: 0.9358 | Generator Loss: 0.6680\n",
      "Epoch 100/100 | Discriminator Loss: 0.7994 | Generator Loss: 0.6931\n"
     ]
    }
   ],
   "source": [
    "train_gan(generator, discriminator, dataloader,batch_size, num_epochs = 100, lr_g=0.001, lr_d=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1058e435",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = torch.randn(size=(100, 20), device=device)\n",
    "with torch.no_grad():\n",
    "    fake_samples = generator(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d7cf0abf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000, 26.8818,  ...,  0.0000, 20.6079,  0.0000],\n",
       "        [36.1054,  0.0000, 30.2158,  ...,  0.0000, 10.6867,  0.0000],\n",
       "        [70.1318,  0.0000, 53.2137,  ...,  0.0000, 38.7306,  0.0000],\n",
       "        ...,\n",
       "        [61.8561,  0.0000,  0.0000,  ...,  6.0459, 32.2684,  0.0000],\n",
       "        [14.7510,  0.0000,  0.0000,  ...,  0.0000, 15.4998,  0.0000],\n",
       "        [50.4327,  0.0000,  0.0000,  ...,  0.0000, 37.0186,  0.0000]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake_samples"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
