{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from utils_timegan import random_generator\n",
    "from utils_timegan import extract_time\n",
    "import timegan_module\n",
    "from timegan_module import Time_GAN_module\n",
    "\n",
    "import os"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = \"AirQualityUCI\"\n",
    "DATA_FILE = \"AirQualityUCI.csv\"\n",
    "#path = os.path.join('',DATA_FOLDER, DATA_FILE)\n",
    "# load csv into df\n",
    "df = pd.read_csv(r'/home/enishiwaki/dowgan-en/notebooks/ems-experiments/AirQualityUCI/AirQualityUCI.csv', \n",
    "                 sep=\";\", \n",
    "                 parse_dates=[['Date', 'Time']],\n",
    "                 decimal=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = \"AirQualityUCI\"\n",
    "DATA_FILE = \"AirQualityUCI.csv\"\n",
    "#path = os.path.join('../ems-experiments',DATA_FOLDER, DATA_FILE)\n",
    "#print(path)\n",
    "# load csv into df\n",
    "df = pd.read_csv(r'/home/enishiwaki/dowgan-en/notebooks/ems-experiments/AirQualityUCI/AirQualityUCI.csv', \n",
    "                 sep=\";\", \n",
    "                 parse_dates=[['Date', 'Time']],\n",
    "                 decimal=',')\n",
    "\n",
    "df = df.iloc[:,0:14]\n",
    "df = df.dropna(thresh=13)\n",
    "df = df.drop(columns=['Date_Time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first entry of df: 2.6\n",
      "shape of dataframe: (9357, 13)\n",
      "first entry of data tensor: 2.5999999046325684\n",
      "shape of data tensor: torch.Size([9357, 13])\n"
     ]
    }
   ],
   "source": [
    "# Turning data into tensor\n",
    "data_tensor = torch.tensor(df.iloc[:,0:13].values, dtype=torch.float32)\n",
    "\n",
    "print(f\"first entry of df: {df.iloc[0,0]}\")\n",
    "print(f\"shape of dataframe: {df.shape}\")\n",
    "print(f\"first entry of data tensor: {data_tensor[0,0]}\")\n",
    "print(f\"shape of data tensor: {data_tensor.shape}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing parameters and classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = dict()\n",
    "parameters['module'] = 'gru' \n",
    "parameters['hidden_dim'] = 40\n",
    "parameters['num_layers'] = 1\n",
    "parameters['iterations'] = 10\n",
    "parameters['epoch'] = 3 \n",
    "parameters['batch_size'] = 100\n",
    "\n",
    "time_points, features = np.asarray(df).shape\n",
    "z_dim = 45\n",
    "dim = 45\n",
    "gamma = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiating every module we're going to train\n",
    "Embedder = Time_GAN_module(input_size=z_dim, output_size=hidden_dim, hidden_dim=hidden_dim, n_layers=num_layers)\n",
    "Recovery = Time_GAN_module(input_size=hidden_dim, output_size=dim, hidden_dim=hidden_dim, n_layers=num_layers)\n",
    "Generator = Time_GAN_module(input_size=dim, output_size=hidden_dim, hidden_dim=hidden_dim, n_layers=num_layers)\n",
    "Supervisor = Time_GAN_module(input_size=hidden_dim, output_size=hidden_dim, hidden_dim=hidden_dim, n_layers=num_layers)\n",
    "Discriminator = Time_GAN_module(input_size=hidden_dim, output_size=1, hidden_dim=hidden_dim, n_layers=num_layers, activation=nn.Identity)\n",
    "\n",
    "# instantiating all optimizers and learning dates\n",
    "embedder_optimizer = optim.Adam(Embedder.parameters(), lr=0.0035)\n",
    "recovery_optimizer = optim.Adam(Recovery.parameters(), lr=0.01)\n",
    "supervisor_optimizer = optim.Adam(Recovery.parameters(), lr=0.001)\n",
    "discriminator_optimizer = optim.Adam(Discriminator.parameters(), lr=0.01)\n",
    "generator_optimizer = optim.Adam(Generator.parameters(), lr=0.01)\n",
    "\n",
    "# instantiating mse loss & Data Loader\n",
    "binary_cross_entropy_loss = nn.BCEWithLogitsLoss()\n",
    "MSE_loss = nn.MSELoss()\n",
    "loader = DataLoader(data_tensor, parameters['batch_size'], shuffle=False)\n",
    "#random_data = random_generator(batch_size=parameters['batch_size'], z_dim=dim, \n",
    "#                                    T_mb=extract_time(data)[0], max_seq_len=extract_time(data)[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9357, 13])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters['batch_size']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataloader: <torch.utils.data.dataloader.DataLoader object at 0x7f0a597f8f10>\n",
      "length of train_dataloader: 94 batches of 100 samples\n",
      "number of examples: 9357 \n",
      "each example containing target shape (number of timepoint): torch.Size([]) \n",
      "and condition shape: torch.Size([])\n"
     ]
    }
   ],
   "source": [
    "print(f\"dataloader: {loader}\")\n",
    "print(f\"length of train_dataloader: {len(loader)} batches of {parameters['batch_size']} samples\")\n",
    "print(f\"number of examples: {len(data_tensor)})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TimeGAN(data, parameters)\n",
    "\n",
    "  checkpoints = {}\n",
    "  \n",
    "  # Embedding Network Training\n",
    "  # Here we train embedding & Recovery network jointly\n",
    "  print('Start Embedding Network Training')\n",
    "  for e in range(epoch): \n",
    "    for batch_index, X in enumerate(loader):\n",
    "                \n",
    "        H, _ = Embedder(X.float())\n",
    "        H = torch.reshape(H, (batch_size, seq_len, hidden_dim))\n",
    "\n",
    "        X_tilde, _ = Recovery(H)\n",
    "        X_tilde = torch.reshape(X_tilde, (batch_size, seq_len, dim))\n",
    "\n",
    "        # constants chosen like in the paper\n",
    "        E_loss0 = 10 * torch.sqrt(MSE_loss(X, X_tilde))  \n",
    "\n",
    "        Embedder.zero_grad()\n",
    "        Recovery.zero_grad()\n",
    "\n",
    "        E_loss0.backward(retain_graph=True)\n",
    "\n",
    "        embedder_optimizer.step()\n",
    "        recovery_optimizer.step()\n",
    "\n",
    "        if e in range(1,epoch) and batch_index == 0:\n",
    "            print('step: '+ str(e) + '/' + str(epoch) + ', e_loss: ' + str(np.sqrt(E_loss0.detach().numpy())))\n",
    "\n",
    "  print('Finish Embedding Network Training')\n",
    "\n",
    "  # Training only with supervised loss\n",
    "  # here the embedder and supervisor are trained jointly\n",
    "  print('Start Training with Supervised Loss Only')\n",
    "  for e in range(epoch): \n",
    "    for batch_index, X in enumerate(loader):\n",
    "\n",
    "        H, _ = Embedder(X.float())\n",
    "        H = torch.reshape(H, (batch_size, seq_len, hidden_dim))\n",
    "\n",
    "        H_hat_supervise, _ = Supervisor(H)\n",
    "        H_hat_supervise = torch.reshape(H_hat_supervise, (batch_size, seq_len, hidden_dim))  \n",
    "\n",
    "        G_loss_S = MSE_loss(H[:,1:,:], H_hat_supervise[:,:-1,:])\n",
    "\n",
    "\n",
    "        Embedder.zero_grad()\n",
    "        Supervisor.zero_grad()\n",
    "\n",
    "        G_loss_S.backward(retain_graph=True)\n",
    "\n",
    "        embedder_optimizer.step()\n",
    "        supervisor_optimizer.step()\n",
    "\n",
    "        if e in range(1,epoch) and batch_index == 0:\n",
    "            print('step: '+ str(e) + '/' + str(epoch) + ', s_loss: ' + str(np.sqrt(G_loss_S.detach().numpy())))\n",
    "\n",
    "  print('Finish Training with Supervised Loss Only')\n",
    "  \n",
    "  \n",
    "  # Joint Training\n",
    "  print('Start Joint Training')\n",
    "  for itt in range(epoch):\n",
    "    # the generator, supervisor and discriminator are trained for an extra two steps\n",
    "    # in the issues on github the author mentions, the marked constant of 0.15 (below)\n",
    "    # had been chosen because it worked well in experiments, and to keep the balance in\n",
    "    # training the generator and discriminator.\n",
    "    for kk in range(2):\n",
    "      X = next(iter(loader))\n",
    "      random_data #= random_generator(batch_size=batch_size, z_dim=dim, \n",
    "                                       #T_mb=extract_time(data)[0], max_seq_len=extract_time(data)[1])\n",
    "        \n",
    "      # Generator Training \n",
    "      ## Train Generator\n",
    "      z = torch.tensor(random_data)\n",
    "      z = z.float()\n",
    "        \n",
    "      e_hat, _ = Generator(z)\n",
    "      e_hat = torch.reshape(e_hat, (batch_size, seq_len, hidden_dim))\n",
    "        \n",
    "      H_hat, _ = Supervisor(e_hat)\n",
    "      H_hat = torch.reshape(H_hat, (batch_size, seq_len, hidden_dim))\n",
    "        \n",
    "      Y_fake = Discriminator(H_hat)\n",
    "      Y_fake = torch.reshape(Y_fake, (batch_size, seq_len, 1))\n",
    "        \n",
    "      x_hat, _ = Recovery(H_hat)\n",
    "      x_hat = torch.reshape(x_hat, (batch_size, seq_len, dim))\n",
    "        \n",
    "      H, _ = Embedder(X.float())\n",
    "      H = torch.reshape(H, (batch_size, seq_len, hidden_dim))\n",
    "\n",
    "      H_hat_supervise, _ = Supervisor(H)\n",
    "      H_hat_supervise = torch.reshape(H_hat_supervise, (batch_size, seq_len, hidden_dim))\n",
    "\n",
    "      Generator.zero_grad()\n",
    "      Supervisor.zero_grad()\n",
    "      Discriminator.zero_grad()\n",
    "      Recovery.zero_grad()\n",
    "\n",
    "      # line 267 of original implementation: \n",
    "      # G_loss_U, G_loss_S, G_loss_V\n",
    "      G_loss_S = MSE_loss(H[:,1:,:], H_hat_supervise[:,:-1,:])\n",
    "      binary_cross_entropy_loss = nn.BCEWithLogitsLoss()\n",
    "      # logits first, then targets\n",
    "      # D_loss_real(Y_real, torch.ones_like(Y_real))\n",
    "      G_loss_U = binary_cross_entropy_loss(Y_fake, torch.ones_like(Y_fake))\n",
    "        \n",
    "      G_loss_V1 = torch.mean(torch.abs((torch.std(x_hat, [0], unbiased = False)) + 1e-6 - (torch.std(X, [0]) + 1e-6)))\n",
    "      G_loss_V2 = torch.mean(torch.abs((torch.mean(x_hat, [0]) - (torch.mean(X, [0])))))\n",
    "      G_loss_V = G_loss_V1 + G_loss_V2\n",
    "        \n",
    "      # doing a backward step for each loss should result in gradients accumulating \n",
    "      # so we should be able to optimize them jointly\n",
    "      G_loss_S.backward(retain_graph=True)#\n",
    "      G_loss_U.backward(retain_graph=True)\n",
    "      G_loss_V.backward(retain_graph=True)#\n",
    "\n",
    "\n",
    "      generator_optimizer.step()\n",
    "      supervisor_optimizer.step()\n",
    "      discriminator_optimizer.step()\n",
    "      # Train Embedder \n",
    "      ## line 270: we only optimize E_loss_T0\n",
    "      ## E_loss_T0 = just mse of x and x_tilde\n",
    "      # but it calls E_solver which optimizes E_loss, which is a sum of \n",
    "      # E_loss0 and 0.1* G_loss_S\n",
    "      MSE_loss = nn.MSELoss()\n",
    "        \n",
    "      H, _ = Embedder(X.float())\n",
    "      H = torch.reshape(H, (batch_size, seq_len, hidden_dim))\n",
    "\n",
    "      X_tilde, _ = Recovery(H)\n",
    "      X_tilde = torch.reshape(X_tilde, (batch_size, seq_len, dim))\n",
    "\n",
    "      E_loss_T0 = MSE_loss(X, X_tilde)\n",
    "      E_loss0 = 10 * torch.sqrt(MSE_loss(X, X_tilde))  \n",
    "        \n",
    "      H_hat_supervise, _ = Supervisor(H)\n",
    "      H_hat_supervise = torch.reshape(H_hat_supervise, (batch_size, seq_len, hidden_dim))  \n",
    "\n",
    "      G_loss_S = MSE_loss(H[:,1:,:], H_hat_supervise[:,:-1,:])\n",
    "      E_loss = E_loss0  + 0.1 * G_loss_S\n",
    "        \n",
    "      G_loss_S.backward(retain_graph=True)\n",
    "      E_loss_T0.backward()\n",
    "        \n",
    "      Embedder.zero_grad()\n",
    "      Recovery.zero_grad()\n",
    "      Supervisor.zero_grad()\n",
    "        \n",
    "      embedder_optimizer.step()\n",
    "      recovery_optimizer.step()\n",
    "      supervisor_optimizer.step()\n",
    "    # train Discriminator\n",
    "    for batch_index, X in enumerate(loader):\n",
    "      random_data #= random_generator(batch_size=batch_size, z_dim=dim, \n",
    "                                       #T_mb=extract_time(data)[0], max_seq_len=extract_time(data)[1])\n",
    "      \n",
    "      z = torch.tensor(random_data)\n",
    "      z = z.float()\n",
    "\n",
    "      H, _ = Embedder(X)\n",
    "      H = torch.reshape(H, (batch_size, seq_len, hidden_dim))\n",
    "\n",
    "      Y_real = Discriminator(H)\n",
    "      Y_real = torch.reshape(Y_real, (batch_size, seq_len, 1))\n",
    "      \n",
    "      e_hat, _ = Generator(z)\n",
    "      e_hat = torch.reshape(e_hat, (batch_size, seq_len, hidden_dim))\n",
    "\n",
    "      Y_fake_e = Discriminator(e_hat)\n",
    "      Y_fake_e = torch.reshape(Y_fake_e, (batch_size, seq_len, 1))\n",
    "        \n",
    "      H_hat, _ = Supervisor(e_hat)\n",
    "      H_hat = torch.reshape(H_hat, (batch_size, seq_len, hidden_dim))\n",
    "        \n",
    "      Y_fake = Discriminator(H_hat)\n",
    "      Y_fake = torch.reshape(Y_fake, (batch_size, seq_len, 1))\n",
    "        \n",
    "      x_hat, _ = Recovery(H_hat)\n",
    "      x_hat = torch.reshape(x_hat, (batch_size, seq_len, dim))\n",
    "\n",
    "      Generator.zero_grad()\n",
    "      Supervisor.zero_grad()\n",
    "      Discriminator.zero_grad()\n",
    "      Recovery.zero_grad()\n",
    "      Embedder.zero_grad()\n",
    "\n",
    "      # logits first, then targets\n",
    "      # D_loss_real(Y_real, torch.ones_like(Y_real))\n",
    "      D_loss_real = nn.BCEWithLogitsLoss()\n",
    "      DLR = D_loss_real(Y_real, torch.ones_like(Y_real))\n",
    "\n",
    "      D_loss_fake = nn.BCEWithLogitsLoss()\n",
    "      DLF = D_loss_fake(Y_fake, torch.zeros_like(Y_fake))\n",
    "\n",
    "      D_loss_fake_e = nn.BCEWithLogitsLoss()\n",
    "      DLF_e = D_loss_fake_e(Y_fake_e, torch.zeros_like(Y_fake_e))\n",
    "\n",
    "      D_loss = DLR + DLF + gamma * DLF_e\n",
    "\n",
    "      # check discriminator loss before updating\n",
    "      check_d_loss = D_loss\n",
    "      # This is the magic number 0.15 we mentioned above. Set exactly like in the original implementation\n",
    "      if (check_d_loss > 0.15):\n",
    "        D_loss.backward(retain_graph=True)\n",
    "        discriminator_optimizer.step()        \n",
    "        \n",
    "      H, _ = Embedder(X.float())\n",
    "      H = torch.reshape(H, (batch_size, seq_len, hidden_dim)) \n",
    "        \n",
    "      X_tilde, _ = Recovery(H)\n",
    "      X_tilde = torch.reshape(X_tilde, (batch_size, seq_len, dim))\n",
    "\n",
    "      \n",
    "      z = torch.tensor(random_data)\n",
    "      z = z.float()\n",
    "        \n",
    "      e_hat, _ = Generator(z)\n",
    "      e_hat = torch.reshape(e_hat, (batch_size, seq_len, hidden_dim))\n",
    "        \n",
    "      H_hat, _ = Supervisor(e_hat)\n",
    "      H_hat = torch.reshape(H_hat, (batch_size, seq_len, hidden_dim))\n",
    "        \n",
    "      Y_fake = Discriminator(H_hat)\n",
    "      Y_fake = torch.reshape(Y_fake, (batch_size, seq_len, 1))\n",
    "        \n",
    "      x_hat, _ = Recovery(H_hat)\n",
    "      x_hat = torch.reshape(x_hat, (batch_size, seq_len, dim))\n",
    "        \n",
    "      H, _ = Embedder(X.float())\n",
    "      H = torch.reshape(H, (batch_size, seq_len, hidden_dim))\n",
    "\n",
    "      H_hat_supervise, _ = Supervisor(H)\n",
    "      H_hat_supervise = torch.reshape(H_hat_supervise, (batch_size, seq_len, hidden_dim))\n",
    "\n",
    "      G_loss_S = MSE_loss(H[:,1:,:], H_hat_supervise[:,:-1,:])\n",
    "      binary_cross_entropy_loss = nn.BCEWithLogitsLoss()\n",
    "      # logits first then targets\n",
    "      G_loss_U = binary_cross_entropy_loss(Y_fake, torch.ones_like(Y_fake))\n",
    "        \n",
    "      G_loss_V1 = torch.mean(torch.abs((torch.std(x_hat, [0], unbiased = False)) + 1e-6 - (torch.std(X, [0]) + 1e-6)))\n",
    "      G_loss_V2 = torch.mean(torch.abs((torch.mean(x_hat, [0]) - (torch.mean(X, [0])))))\n",
    "      G_loss_V = G_loss_V1 + G_loss_V2\n",
    "    \n",
    "      E_loss_T0 = MSE_loss(X, X_tilde)\n",
    "      E_loss0 = 10 * torch.sqrt(MSE_loss(X, X_tilde))  \n",
    "      E_loss = E_loss0  + 0.1 * G_loss_S\n",
    "        \n",
    "      # doing a backward step for each loss should result in gradients accumulating \n",
    "      # so we should be able to optimize them jointly\n",
    "      G_loss_S.backward(retain_graph=True)#\n",
    "      G_loss_U.backward(retain_graph=True)\n",
    "      G_loss_V.backward(retain_graph=True)#\n",
    "      E_loss.backward()\n",
    "\n",
    "      generator_optimizer.step()\n",
    "      supervisor_optimizer.step()\n",
    "      embedder_optimizer.step()\n",
    "      recovery_optimizer.step()\n",
    "            \n",
    "      print('step: '+ str(itt) + '/' + str(epoch) + \n",
    "            ', D_loss: ' + str(D_loss.detach().numpy()) +\n",
    "            ', G_loss_U: ' + str(G_loss_U.detach().numpy()) + \n",
    "            ', G_loss_S: ' + str(G_loss_S.detach().numpy()) + \n",
    "            ', E_loss_t0: ' + str(np.sqrt(E_loss0.detach().numpy())))\n",
    "         \n",
    "\n",
    "      \n",
    "      random_test = random_generator(1, dim, extract_time(data)[0], extract_time(data)[1])        \n",
    "      test_sample = Generator(torch.tensor(random_generator(1, dim, extract_time(data)[0], extract_time(data)[1])).float())[0]      \n",
    "      test_sample = torch.reshape(test_sample, (1, seq_len, hidden_dim))\n",
    "      test_recovery = Recovery(test_sample)\n",
    "      test_recovery = torch.reshape(test_recovery[0], (1, seq_len, dim))\n",
    "      fig, ax = plt.subplots()\n",
    "      ax1 = plt.plot(test_recovery[0].detach().numpy())\n",
    "      plt.show()\n",
    "      \n",
    "      if itt % 2:\n",
    "        checkpoints[itt] = [Generator.state_dict(), Discriminator.state_dict(), Embedder.state_dict(), Recovery.state_dict,\n",
    "                    Supervisor.state_dict()]\n",
    "             \n",
    "  print('Finish Joint Training')\n",
    "                \n",
    "  return Generator, Embedder, Supervisor, Recovery, Discriminator, checkpoints"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dowgan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
